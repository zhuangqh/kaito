name: "e2e-base-setup"
description: "Composite action for e2e base setup"

inputs:
  git_sha:
    description: "Git commit SHA"
    required: true
  node_provisioner:
    description: "Provisioner type"
    required: false
    default: "gpuprovisioner"
  tag:
    description: "Tag"
    required: false
  isRelease:
    description: "Is Release"
    required: false
    default: "false"
  registry:
    description: "Registry"
    required: false
  region:
    description: "the azure location to run the e2e test in"
    required: false
    default: "eastus"
  k8s_version:
    description: "Kubernetes version"
    required: false
    default: "1.33.6"

outputs:
  cluster_name:
    description: "Cluster name"
  registry:
    description: "Registry value"

env:
  GO_VERSION: "1.24"

runs:
  using: "composite"
  steps:
    - name: Checkout
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        ref: ${{ inputs.git_sha }}

    - name: Set e2e Resource and Cluster Name
      shell: bash
      run: |
        ID="ri${{ github.run_id }}rn${{ github.run_number }}ra${{ github.run_attempt }}"

        echo "VERSION=${ID}" >> $GITHUB_ENV
        echo "CLUSTER_NAME=kaito${ID}" >> $GITHUB_ENV
        echo "REGISTRY=kaito${ID}.azurecr.io" >> $GITHUB_ENV

    - name: Set Registry
      if: ${{ inputs.isRelease == 'true' }}
      shell: bash
      run: |
        echo "REGISTRY=${{ inputs.registry }}" >> $GITHUB_ENV
        echo "VERSION=$(echo ${{ inputs.tag }} | tr -d v)" >> $GITHUB_ENV

    - name: Remove existing Go modules directory
      shell: bash
      run: sudo rm -rf ~/go/pkg/mod

    - name: Set up Go 1.24
      uses: actions/setup-go@v5.2.0
      with:
        go-version: "1.24"

    - name: Install Azure CLI latest
      shell: bash
      run: |
        if ! which az > /dev/null; then
          echo "Azure CLI not found. Installing..."
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
        else
          echo "Azure CLI already installed."
        fi

    - name: Azure CLI Login
      shell: bash
      run: |
        az login --identity

    - name: Install Helm
      uses: azure/setup-helm@v4

    - name: Create Resource Group
      shell: bash
      run: |
        make create-rg
      env:
        AZURE_RESOURCE_GROUP: ${{ env.CLUSTER_NAME }}

    - name: Create ACR
      shell: bash
      run: |
          az acr create --name "$AZURE_ACR_NAME" --resource-group "$AZURE_RESOURCE_GROUP" --location "$AZURE_LOCATION" --sku Standard --admin-enabled -o none
          az acr login  --name "$AZURE_ACR_NAME"
          # Enable anonymous pull and public network access to make it easier for testing
          az acr update --name "$AZURE_ACR_NAME" --anonymous-pull-enabled true
      env:
        AZURE_RESOURCE_GROUP: ${{ env.CLUSTER_NAME }}
        AZURE_ACR_NAME: ${{ env.CLUSTER_NAME }}
        AZURE_LOCATION: ${{ inputs.region }}

    - name: Create Azure Identity
      uses: azure/CLI@v2.1.0
      with:
        inlineScript: |
          az identity create --name "${{ inputs.node_provisioner }}Identity" --resource-group "${{ env.CLUSTER_NAME }}"

    - name: Generate APIs
      shell: bash
      run: |
        make generate

    - name: Install HuggingFace CLI
      shell: bash
      run: |
        echo "Installing HuggingFace CLI..."
        if ! pip install huggingface-hub; then
          pip install huggingface-hub --break-system-packages
        fi

    - name: Install ORAS CLI
      shell: bash
      run: |
        if ! command -v oras &> /dev/null; then
          echo "Installing ORAS CLI..."
          ORAS_VERSION=$(curl -s https://api.github.com/repos/oras-project/oras/releases/latest | grep 'tag_name' | cut -d\" -f4)
          curl -LO "https://github.com/oras-project/oras/releases/download/${ORAS_VERSION}/oras_${ORAS_VERSION#v}_linux_amd64.tar.gz"
          mkdir -p oras-install/
          tar -zxf "oras_${ORAS_VERSION#v}_linux_amd64.tar.gz" -C oras-install/
          sudo mv oras-install/oras /usr/local/bin/
          rm -rf oras-install "oras_${ORAS_VERSION#v}_linux_amd64.tar.gz"
        else
          echo "ORAS CLI already installed."
        fi
        oras version

    - name: build KAITO base image
      shell: bash
      if: ${{ inputs.build_kaito_base_image == 'true' }}
      run: |
        docker buildx build \
          --builder img-builder \
          --build-arg VERSION=$VERSION \
          --build-arg MODEL_TYPE=text-generation \
          --output type=image,name=$REGISTRY/kaito-base:$VERSION,push=true,compression=zstd \
          -f ./docker/presets/models/tfs/Dockerfile .

    - name: Build preset images
      shell: bash
      run: |
        # For building newly introduced preset images or bumping the tags of existing ones.
        # Without this step, the Workspace controller during e2e test will attempt to pull
        # non-existent images from remote registry, leading to test failures.
        python3 .github/determine_missing_preset_images.py | jq -c '.[]' | while read -r model; do
          export MODEL_NAME=$(echo "$model" | jq -r '.name')

          # Push preset model weights as OCI artifact to ACR using ORAS
          if [ "$MODEL_NAME" != "base" ]; then
            export MODEL_VERSION_URL=$(echo "$model" | jq -r '.version')

            # Use shared script to build and push OCI artifact
            docker/presets/models/tfs/build_oci_artifact.sh \
              "$MODEL_VERSION_URL" \
              "kaito-$MODEL_NAME" \
              "${{ env.REGISTRY }}" \
              "${{ env.VERSION }}" \
              "$HF_TOKEN"
          fi

          # Override image properties in supported_models.yaml for testing purposes.
          # The updated supported_models.yaml will be read by the Workspace controller via
          # go:embed in presets/workspace/models/metadata.go.
          yq '(.models[] | select(.name == strenv(MODEL_NAME)) | .registry) = "${{ env.REGISTRY }}"' -i presets/workspace/models/supported_models.yaml
          yq '(.models[] | select(.name == strenv(MODEL_NAME)) | .tag) = "${{ env.VERSION }}"' -i presets/workspace/models/supported_models.yaml
        done

        cat presets/workspace/models/supported_models.yaml
      env:
        WEIGHTS_DIR: "/datadrive"

    - name: build KAITO image
      if: ${{ inputs.isRelease == 'false' }}
      shell: bash
      run: |
        make docker-build-workspace
      env:
        ARCH: amd64

    - name: create cluster
      shell: bash
      run: |
        if [ "${{ inputs.node_provisioner }}" == "gpuprovisioner" ]; then
          make create-aks-cluster
        else
          make create-aks-cluster-for-karpenter
        fi
      env:
        AZURE_ACR_NAME: ${{ env.CLUSTER_NAME }}
        AZURE_RESOURCE_GROUP: ${{ env.CLUSTER_NAME }}
        AZURE_CLUSTER_NAME: ${{ env.CLUSTER_NAME }}
        AZURE_LOCATION: ${{ inputs.region }}
        AKS_K8S_VERSION: ${{ inputs.k8s_version }}

    - name: Create Identities and Permissions for ${{ inputs.node_provisioner }}
      shell: bash
      run: |
        AZURE_SUBSCRIPTION_ID=$E2E_SUBSCRIPTION_ID \
        make generate-identities
      env:
        AZURE_RESOURCE_GROUP: ${{ env.CLUSTER_NAME }}
        AZURE_CLUSTER_NAME: ${{ env.CLUSTER_NAME }}
        TEST_SUITE: ${{ inputs.node_provisioner }}

    - name: Install gpu-provisioner helm chart
      if: ${{ inputs.node_provisioner == 'gpuprovisioner' }}
      shell: bash
      run: |
        AZURE_SUBSCRIPTION_ID=$E2E_SUBSCRIPTION_ID \
        make gpu-provisioner-helm
      env:
        AZURE_RESOURCE_GROUP: ${{ env.CLUSTER_NAME }}
        AZURE_CLUSTER_NAME: ${{ env.CLUSTER_NAME }}

    - name: Install KAITO Workspace helm chart
      shell: bash
      run: |
        make az-patch-install-helm
        kubectl wait --for=condition=available deploy "kaito-workspace" -n kaito-workspace --timeout=300s
      env:
        AZURE_RESOURCE_GROUP: ${{ env.CLUSTER_NAME }}
        AZURE_CLUSTER_NAME: ${{ env.CLUSTER_NAME }}
        REGISTRY: ${{ env.REGISTRY }}
        VERSION: ${{ env.VERSION }}
        TEST_SUITE: ${{ inputs.node_provisioner }}
        HELM_INSTALL_EXTRA_ARGS: "--set featureGates.gatewayAPIInferenceExtension=true --set featureGates.enableInferenceSetController=true"

    - name: Set up E2E ACR Credentials and Secret
      shell: bash
      run: |
        ACR_USERNAME=$(az acr credential show --name "${{ env.CLUSTER_NAME }}" --resource-group "${{ env.CLUSTER_NAME }}" --query "username" -o tsv)
        ACR_PASSWORD=$(az acr credential show --name "${{ env.CLUSTER_NAME }}" --resource-group "${{ env.CLUSTER_NAME }}" --query "passwords[0].value" -o tsv)

        if [ -z "$ACR_USERNAME" ] || [ -z "$ACR_PASSWORD" ]; then
          echo "Failed to retrieve ACR credentials"
          exit 1
        fi

        kubectl create secret docker-registry "${{ env.CLUSTER_NAME }}-acr-secret" \
          --docker-server="${{ env.CLUSTER_NAME }}.azurecr.io" \
          --docker-username="${ACR_USERNAME}" \
          --docker-password="${ACR_PASSWORD}"

    # Add Private-Hosted ACR secret for private models like llama
    - name: Add Private-Hosted ACR Secret Credentials
      shell: bash
      run: |
        E2E_AMRT_SECRET_NAME=$(echo "$E2E_AMRT_SECRET_NAME" | sed 's/[\"'\'']//g')
        if kubectl get secret "$E2E_AMRT_SECRET_NAME" >/dev/null 2>&1; then
          echo "Secret $E2E_AMRT_SECRET_NAME already exists. Skipping creation."
        else
          kubectl create secret docker-registry "$E2E_AMRT_SECRET_NAME" \
            --docker-server="$E2E_ACR_AMRT_USERNAME.azurecr.io" \
            --docker-username="$E2E_ACR_AMRT_USERNAME" \
            --docker-password="$E2E_ACR_AMRT_PASSWORD"
          echo "Secret $E2E_AMRT_SECRET_NAME created successfully."
        fi
