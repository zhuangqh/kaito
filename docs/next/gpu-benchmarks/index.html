<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-gpu-benchmarks" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">GPU Benchmarks | KAITO</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://kaito-project.github.io/kaito/docs/img/kaito-logo.png"><meta data-rh="true" name="twitter:image" content="https://kaito-project.github.io/kaito/docs/img/kaito-logo.png"><meta data-rh="true" property="og:url" content="https://kaito-project.github.io/kaito/docs/next/gpu-benchmarks"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="GPU Benchmarks | KAITO"><meta data-rh="true" name="description" content="These benchmarks help users choose the optimal GPU SKU for running their AI models with KAITO. We compare performance characteristics across different GPU types to guide cost-effective hardware selection."><meta data-rh="true" property="og:description" content="These benchmarks help users choose the optimal GPU SKU for running their AI models with KAITO. We compare performance characteristics across different GPU types to guide cost-effective hardware selection."><link data-rh="true" rel="icon" href="/kaito/docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kaito-project.github.io/kaito/docs/next/gpu-benchmarks"><link data-rh="true" rel="alternate" href="https://kaito-project.github.io/kaito/docs/next/gpu-benchmarks" hreflang="en"><link data-rh="true" rel="alternate" href="https://kaito-project.github.io/kaito/docs/next/gpu-benchmarks" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://DUMMY_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"GPU Benchmarks","item":"https://kaito-project.github.io/kaito/docs/next/gpu-benchmarks"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="KAITO" href="/kaito/docs/opensearch.xml">

<meta name="algolia-site-verification" content="DUMMY_SITE_VERIFICATION"><link rel="stylesheet" href="/kaito/docs/assets/css/styles.9700f45c.css">
<script src="/kaito/docs/assets/js/runtime~main.d71ae84e.js" defer="defer"></script>
<script src="/kaito/docs/assets/js/main.5a9d3354.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><link rel="preload" as="image" href="/kaito/docs/img/kaito-logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="theme-announcement-bar announcementBar_mb4j" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">⭐️ If you like KAITO, please give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaito-project/kaito">GitHub</a>!</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/kaito/docs/"><div class="navbar__logo"><img src="/kaito/docs/img/kaito-logo.png" alt="KAITO Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/kaito/docs/img/kaito-logo.png" alt="KAITO Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">KAITO</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-current="page" class="navbar__link active" aria-haspopup="true" aria-expanded="false" role="button" href="/kaito/docs/next/gpu-benchmarks">Next</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/kaito/docs/next/gpu-benchmarks">Next</a></li><li><a class="dropdown__link" href="/kaito/docs/">v0.6.x</a></li><li><a class="dropdown__link" href="/kaito/docs/v0.5.x/">v0.5.x</a></li></ul></div><a href="https://github.com/kaito-project/kaito" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/kaito/docs/next/">Getting Started</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/installation">Installation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/quick-start">Quick Start</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/presets">Presets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/usage">Usage</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/faq">FAQ</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/kaito/docs/next/azure">Cloud Providers</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/azure">Azure Setup</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/aws">AWS Setup</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/kaito/docs/next/inference">Features</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/inference">Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/multi-node-inference">Multi-Node Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/tuning">Fine Tuning</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/kaito/docs/next/rag">Retrieval-Augmented Generation (RAG)</a><button aria-label="Expand sidebar category &#x27;Retrieval-Augmented Generation (RAG)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/custom-model">Custom Model Integration</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/tool-calling">Tool Calling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/model-as-oci-artifacts">Model As OCI Artifacts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/headlamp-kaito">Headlamp KAITO</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/kaito/docs/next/aikit">Integrations</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/aikit">AIKit Integration with KAITO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/gateway-api-inference-extension">Gateway API Inference Extension</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/kaito/docs/next/monitoring">Operations</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/monitoring">Monitoring</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/kaito-oom-prevention">OOM Prevention</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/kaito-on-byo-gpu-nodes">Bring Your Own GPU Nodes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/kaito/docs/next/gpu-benchmarks">GPU Benchmarks</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/kaito/docs/next/contributing">Contributing</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/contributing">Contributing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/preset-onboarding">Preset onboarding</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/kaito/docs/next/proposals">Proposals</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>This is unreleased documentation for <!-- -->KAITO<!-- --> <b>Next</b> version.</div><div class="margin-top--md">For up-to-date documentation, see the <b><a href="/kaito/docs/">latest version</a></b> (<!-- -->v0.6.x<!-- -->).</div></div><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/kaito/docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Operations</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">GPU Benchmarks</span></li></ul></nav><span class="theme-doc-version-badge badge badge--secondary">Version: Next</span><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>GPU Benchmarks</h1></header><p>These benchmarks help users choose the optimal GPU SKU for running their AI models with KAITO. We compare performance characteristics across different GPU types to guide cost-effective hardware selection.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup">Test Setup<a href="#test-setup" class="hash-link" aria-label="Direct link to Test Setup" title="Direct link to Test Setup">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="infrastructure-configuration">Infrastructure Configuration<a href="#infrastructure-configuration" class="hash-link" aria-label="Direct link to Infrastructure Configuration" title="Direct link to Infrastructure Configuration">​</a></h3>
<p>We established a dedicated Kubernetes cluster with KAITO deployed to conduct comprehensive GPU performance benchmarks. The testing infrastructure was configured to evaluate three distinct GPU types: NVIDIA A10, A100, and H100, each representing different performance tiers and cost points in the GPU ecosystem.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-selection-rationale">GPU Selection Rationale<a href="#gpu-selection-rationale" class="hash-link" aria-label="Direct link to GPU Selection Rationale" title="Direct link to GPU Selection Rationale">​</a></h3>
<p>Due to memory and compute constraints, the A10 GPU was evaluated separately using lighter model configurations, as it lacks sufficient resources to handle the larger workloads tested on the A100 and H100 GPUs. The primary comparison focused on A100 and H100 performance characteristics using two representative models: <code>phi-4-mini-instruct</code> and <code>llama-3.1-8b-instruct</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-framework">Benchmark Framework<a href="#benchmark-framework" class="hash-link" aria-label="Direct link to Benchmark Framework" title="Direct link to Benchmark Framework">​</a></h3>
<p>All performance tests utilized <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> by running the official <code>benchmark_serving.py</code> <a href="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py" target="_blank" rel="noopener noreferrer">script</a>, which provides standardized metrics for evaluating inference server performance. This tool measures critical performance indicators including:</p>
<ul>
<li><strong>Time to First Token (TTFT)</strong>: How long the user must wait until the model begins to deliver a response.</li>
<li><strong>Inter-Token Latency (ITL)</strong>: How fast the model generates subsequent tokens after it begins its response.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-configurations">Model Configurations<a href="#model-configurations" class="hash-link" aria-label="Direct link to Model Configurations" title="Direct link to Model Configurations">​</a></h3>
<p>We tested the models <a href="https://huggingface.co/microsoft/Phi-4-mini-instruct" target="_blank" rel="noopener noreferrer">Phi-4-mini-instruct</a> and <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct" target="_blank" rel="noopener noreferrer">Llama-3.1-8B-Instruct</a>. The following fields were configured:</p>
<ul>
<li><strong>Requests Per Second (RPS)</strong>: Total throughput capacity with latency in powers of two, such as 1, 2, 4, etc. up to 64 for larger GPUs, indicated by the <code>--request-rate</code> flag.</li>
<li><strong>Max Concurrency</strong>: The maximum number of simultaneous requests the server can handle, configured with the <code>--max-concurrency</code> flag. It was set to 200 for the A10 and H100 tests and 50 for the A10 tests. These values were selected in order to set a ceiling high enough that it will not bottleneck the GPU.</li>
<li><strong>Random Input Length</strong>: The length of the input prompts, configured with the <code>--random-input-len</code> flag. This value was increased for larger models/GPUs that could handle an increased workload. A smaller value could represent a conversation workload, while a larger value could represent a document or source code.</li>
<li><strong>Random Output Length</strong>: The length of the expected output responses, configured with the <code>--random-output-len</code> flag.</li>
<li><strong>Number of prompts</strong>: We configured each test to run for 60 seconds, so with the number of prompts was set to <code>$RATE * 60</code>.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="phi-4-mini-instruct-model">Phi-4 Mini Instruct Model<a href="#phi-4-mini-instruct-model" class="hash-link" aria-label="Direct link to Phi-4 Mini Instruct Model" title="Direct link to Phi-4 Mini Instruct Model">​</a></h4>
<p>The <code>microsoft/Phi-4-mini-instruct</code> model was tested with the following configuration with</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python benchmark_serving.py </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--backend</span><span class="token plain"> vllm </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--model</span><span class="token plain"> microsoft/Phi-4-mini-instruct </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --served-model-name phi-4-mini-instruct </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --base-url http://localhost:8000 </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--endpoint</span><span class="token plain"> /v1/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --num-prompts </span><span class="token variable" style="color:#36acaa">$NUM_PROMPTS</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --request-rate </span><span class="token variable" style="color:#36acaa">$RATE</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --max-concurrency </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --random-input-len </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --random-output-len </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --dataset-name random </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --ignore-eos </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --save-result </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --result-dir </span><span class="token string" style="color:#e3116c">&quot;./out/</span><span class="token string variable" style="color:#36acaa">$GPU</span><span class="token string" style="color:#e3116c">&quot;</span><br></span></code></pre></div></div>
<p><strong>Test Parameters:</strong></p>
<ul>
<li><strong>Input Token Length</strong>: 200 tokens (simulating medium-length user queries)</li>
<li><strong>Output Token Length</strong>: 200 tokens (representing typical conversational responses)</li>
<li><strong>Use Case</strong>: Optimized for interactive chat applications and content generation</li>
<li><strong>Load Pattern</strong>: Variable request rates from 1 to 64 QPS</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llama-31-8b-instruct-model">Llama 3.1 8B Instruct Model<a href="#llama-31-8b-instruct-model" class="hash-link" aria-label="Direct link to Llama 3.1 8B Instruct Model" title="Direct link to Llama 3.1 8B Instruct Model">​</a></h4>
<p>The <code>meta-llama/Llama-3.1-8B-Instruct</code> model was configured for high-throughput, short-response scenarios:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python benchmark_serving.py </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--backend</span><span class="token plain"> vllm </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--model</span><span class="token plain"> meta-llama/Llama-3.1-8B-Instruct </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --served-model-name llama-3.1-8b-instruct </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --base-url http://localhost:8000 </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--endpoint</span><span class="token plain"> /v1/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --num-prompts </span><span class="token variable" style="color:#36acaa">$NUM_PROMPTS</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --request-rate </span><span class="token variable" style="color:#36acaa">$RATE</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --max-concurrency </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --random-input-len </span><span class="token number" style="color:#36acaa">1000</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --random-output-len </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --dataset-name random </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --ignore-eos </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --save-result </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --result-dir </span><span class="token string" style="color:#e3116c">&quot;./out/</span><span class="token string variable" style="color:#36acaa">$GPU</span><span class="token string" style="color:#e3116c">&quot;</span><br></span></code></pre></div></div>
<p><strong>Test Parameters:</strong></p>
<ul>
<li><strong>Input Token Length</strong>: 1000 tokens</li>
<li><strong>Output Token Length</strong>: 200 tokens</li>
<li><strong>Use Case</strong>: Optimized for large context understanding with minimal output generation</li>
<li><strong>Load Pattern</strong>: Variable request rates from 1 to 64 QPS</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="a10-gpu">A10 GPU<a href="#a10-gpu" class="hash-link" aria-label="Direct link to A10 GPU" title="Direct link to A10 GPU">​</a></h4>
<p>Since the A10 GPU was not powerful enough to run larger models, higher input/output lengths, or higher QPS, we tested it individually with lower settings as to not skew the results of the other GPUs.</p>
<ul>
<li>The QPS ranged in powers of two from 1 to 16 instead of 64.</li>
<li>For Phi 4 Mini and Llama 3.1 8B, the max concurrency was set to 50 instead of 200.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="testing-methodology">Testing Methodology<a href="#testing-methodology" class="hash-link" aria-label="Direct link to Testing Methodology" title="Direct link to Testing Methodology">​</a></h3>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-results">Benchmark Results<a href="#benchmark-results" class="hash-link" aria-label="Direct link to Benchmark Results" title="Direct link to Benchmark Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a10-gpu-results">A10 GPU results<a href="#a10-gpu-results" class="hash-link" aria-label="Direct link to A10 GPU results" title="Direct link to A10 GPU results">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="time-to-first-token">Time to First Token<a href="#time-to-first-token" class="hash-link" aria-label="Direct link to Time to First Token" title="Direct link to Time to First Token">​</a></h4>
<p><img decoding="async" loading="lazy" alt="img" src="/kaito/docs/assets/images/a10-ttft-6a0f08628202149cbbc5d4c0f64587f6.png" width="1983" height="1323" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="inter-token-latency">Inter-token Latency<a href="#inter-token-latency" class="hash-link" aria-label="Direct link to Inter-token Latency" title="Direct link to Inter-token Latency">​</a></h4>
<p><img decoding="async" loading="lazy" alt="img" src="/kaito/docs/assets/images/a10-itl-1b977f788d00d992c70691b67b898961.png" width="1983" height="1322" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phi-4-mini-comparison">Phi 4 Mini Comparison<a href="#phi-4-mini-comparison" class="hash-link" aria-label="Direct link to Phi 4 Mini Comparison" title="Direct link to Phi 4 Mini Comparison">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="time-to-first-token-1">Time to First Token<a href="#time-to-first-token-1" class="hash-link" aria-label="Direct link to Time to First Token" title="Direct link to Time to First Token">​</a></h4>
<p><img decoding="async" loading="lazy" alt="img" src="/kaito/docs/assets/images/phi-4-mini-ttft-634b634c0a1ac5b17bdfa73c5672765e.png" width="1983" height="1323" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="inter-token-latency-1">Inter-token Latency<a href="#inter-token-latency-1" class="hash-link" aria-label="Direct link to Inter-token Latency" title="Direct link to Inter-token Latency">​</a></h4>
<p><img decoding="async" loading="lazy" alt="img" src="/kaito/docs/assets/images/phi-4-mini-itl-60edef73d48b708a71da51d84ec263f6.png" width="1982" height="1323" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-31-8b-comparison">Llama 3.1 8B comparison<a href="#llama-31-8b-comparison" class="hash-link" aria-label="Direct link to Llama 3.1 8B comparison" title="Direct link to Llama 3.1 8B comparison">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="time-to-first-token-2">Time to First Token<a href="#time-to-first-token-2" class="hash-link" aria-label="Direct link to Time to First Token" title="Direct link to Time to First Token">​</a></h4>
<p><img decoding="async" loading="lazy" alt="img" src="/kaito/docs/assets/images/llama-3.1-8b-ttft-1e607c3bff9f417f70f7ceb65bc096ac.png" width="1982" height="1323" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="inter-token-latency-2">Inter-Token Latency<a href="#inter-token-latency-2" class="hash-link" aria-label="Direct link to Inter-Token Latency" title="Direct link to Inter-Token Latency">​</a></h4>
<p><img decoding="async" loading="lazy" alt="img" src="/kaito/docs/assets/images/llama-3.1-8b-itl-c171e4e2b73e195b776c948a3ecb6c35.png" width="1982" height="1323" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-analysis">Performance Analysis<a href="#performance-analysis" class="hash-link" aria-label="Direct link to Performance Analysis" title="Direct link to Performance Analysis">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a10-gpu-performance">A10 GPU Performance<a href="#a10-gpu-performance" class="hash-link" aria-label="Direct link to A10 GPU Performance" title="Direct link to A10 GPU Performance">​</a></h3>
<p>The A10 GPU performas well on Phi 4 Mini but is not suited for larger models such as Llama 3.1 8B.</p>
<p><strong>Phi 4 Mini on A10:</strong></p>
<ul>
<li>TTFT remains stable and fast from (19-35ms)</li>
<li>ITL is consistent and low (4.78-6.07ms) but begins to grow as request rates increase.</li>
</ul>
<p><strong>Llama 3.1 8B on A10:</strong></p>
<ul>
<li>Shows dramatically higher TTFT (671-20,344) and would result in a considerable wait time.</li>
<li>ITL grows but remains consistently fast despite the extremely long TTFT.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a100-vs-h100-comparison">A100 vs H100 Comparison<a href="#a100-vs-h100-comparison" class="hash-link" aria-label="Direct link to A100 vs H100 Comparison" title="Direct link to A100 vs H100 Comparison">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="phi-4-mini-performance">Phi 4 Mini Performance<a href="#phi-4-mini-performance" class="hash-link" aria-label="Direct link to Phi 4 Mini Performance" title="Direct link to Phi 4 Mini Performance">​</a></h4>
<ul>
<li>The H100 almost always has a consistent TTFT of around 600-700 ms, which is surprisingly higher than that of the A100. This is likely because the H100 is meant for larger workloads and a smaller model like this is not optimized to take advantage of its bandwidth.</li>
<li>The ITL of the H100 is, however, is also consistently lower across all load levels than the A100, which is as expected based as the H100 is a more powerful GPU.</li>
<li>In terms of scaling, the H100 maintains stable performance up to 32 QPS, then shows degradation at 64 QPS, and the A100 shows more gradual degradation but reaches saturation earlier.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llama-31-8b-performance">Llama 3.1 8B Performance<a href="#llama-31-8b-performance" class="hash-link" aria-label="Direct link to Llama 3.1 8B Performance" title="Direct link to Llama 3.1 8B Performance">​</a></h4>
<ul>
<li>Both GPUs show very similar performance characteristics for this model as TTFT and ITL metrics are very close between the two GPUs.</li>
<li>Both GPUs handle up to 8 QPS with excellent performance and significant performance jump occurs at 16 QPS, which likely reaches a threshold where batching optimizations kick in.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="recommendations">Recommendations<a href="#recommendations" class="hash-link" aria-label="Direct link to Recommendations" title="Direct link to Recommendations">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cost-performance-optimization">Cost-Performance Optimization<a href="#cost-performance-optimization" class="hash-link" aria-label="Direct link to Cost-Performance Optimization" title="Direct link to Cost-Performance Optimization">​</a></h3>
<ol>
<li>
<p><strong>For Phi 4 Mini workloads:</strong></p>
<ul>
<li><strong>H100</strong>: Recommended for high-throughput production environments requiring low latency</li>
<li><strong>A100</strong>: Cost-effective option for moderate workloads where slight latency increases are acceptable</li>
<li><strong>A10</strong>: Suitable for development, testing, or low-volume production use cases</li>
</ul>
</li>
<li>
<p><strong>For Llama 3.1 8B workloads:</strong></p>
<ul>
<li><strong>A100 and H100</strong>: Performance parity makes the A100 the more cost-effective choice</li>
<li><strong>A10</strong>: Can handle the model but limited to short output scenarios</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="workload-specific-guidance">Workload-Specific Guidance<a href="#workload-specific-guidance" class="hash-link" aria-label="Direct link to Workload-Specific Guidance" title="Direct link to Workload-Specific Guidance">​</a></h3>
<p><strong>High-Throughput Scenarios (&gt;32 QPS):</strong></p>
<ul>
<li>H100 for Phi 4 Mini due to superior scaling characteristics</li>
<li>Either A100 or H100 for Llama 3.1 8B, with A100 offering better cost efficiency</li>
</ul>
<p><strong>Latency-Sensitive Applications:</strong></p>
<ul>
<li>H100 provides consistently lower TTFT and ITL for Phi 4 Mini</li>
<li>A100 and H100 perform similarly for Llama 3.1 8B</li>
</ul>
<p><strong>Budget-Conscious Deployments:</strong></p>
<ul>
<li>A10 for light workloads and development</li>
<li>A100 for production workloads where H100 performance gains don&#x27;t justify the cost premium</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-considerations">Scaling Considerations<a href="#scaling-considerations" class="hash-link" aria-label="Direct link to Scaling Considerations" title="Direct link to Scaling Considerations">​</a></h3>
<p>The benchmark results suggest optimal operating ranges:</p>
<ul>
<li><strong>Low-load optimal</strong>: 1-8 QPS for consistent, predictable performance</li>
<li><strong>High-throughput optimal</strong>: 16-32 QPS where batching optimizations provide maximum efficiency</li>
<li><strong>Saturation point</strong>: Beyond 32-64 QPS, performance gains diminish and latency increases</li>
</ul>
<p>KAITO is currently adding a feature to distribute the load across model servers when QPS is high to mitigate these issues.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>We are currently working on adding automatic scaling across model servers when QPS is high.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>These benchmarks demonstrate that GPU selection should be based on specific workload requirements and cost considerations. While the H100 shows superior performance for certain models like Phi 4 Mini, the A100 provides excellent value for models like Llama 3.1 8B where performance parity exists. The A10 serves as an entry-level option for development and light production workloads.</p>
<p>For optimal cost-performance in production environments, we recommend starting with A100 GPUs and scaling to H100 only when the specific performance requirements and throughput demands justify the additional cost.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/kaito-project/kaito/tree/main/website/docs/gpu-benchmarks.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/kaito/docs/next/kaito-on-byo-gpu-nodes"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Bring Your Own GPU Nodes</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/kaito/docs/next/contributing"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Contributing</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#test-setup" class="table-of-contents__link toc-highlight">Test Setup</a><ul><li><a href="#infrastructure-configuration" class="table-of-contents__link toc-highlight">Infrastructure Configuration</a></li><li><a href="#gpu-selection-rationale" class="table-of-contents__link toc-highlight">GPU Selection Rationale</a></li><li><a href="#benchmark-framework" class="table-of-contents__link toc-highlight">Benchmark Framework</a></li><li><a href="#model-configurations" class="table-of-contents__link toc-highlight">Model Configurations</a></li><li><a href="#testing-methodology" class="table-of-contents__link toc-highlight">Testing Methodology</a></li></ul></li><li><a href="#benchmark-results" class="table-of-contents__link toc-highlight">Benchmark Results</a><ul><li><a href="#a10-gpu-results" class="table-of-contents__link toc-highlight">A10 GPU results</a></li><li><a href="#phi-4-mini-comparison" class="table-of-contents__link toc-highlight">Phi 4 Mini Comparison</a></li><li><a href="#llama-31-8b-comparison" class="table-of-contents__link toc-highlight">Llama 3.1 8B comparison</a></li></ul></li><li><a href="#performance-analysis" class="table-of-contents__link toc-highlight">Performance Analysis</a><ul><li><a href="#a10-gpu-performance" class="table-of-contents__link toc-highlight">A10 GPU Performance</a></li><li><a href="#a100-vs-h100-comparison" class="table-of-contents__link toc-highlight">A100 vs H100 Comparison</a></li></ul></li><li><a href="#recommendations" class="table-of-contents__link toc-highlight">Recommendations</a><ul><li><a href="#cost-performance-optimization" class="table-of-contents__link toc-highlight">Cost-Performance Optimization</a></li><li><a href="#workload-specific-guidance" class="table-of-contents__link toc-highlight">Workload-Specific Guidance</a></li><li><a href="#scaling-considerations" class="table-of-contents__link toc-highlight">Scaling Considerations</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Documentation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/kaito/docs/">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/kaito/docs/installation">Installation</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/kaito-project/kaito" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://cloud-native.slack.com/archives/C09B4EWCZ5M" target="_blank" rel="noopener noreferrer" class="footer__link-item">Slack<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 KAITO Project, Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>