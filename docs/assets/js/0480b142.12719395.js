"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[70],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const o={},i=r.createContext(o);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(i.Provider,{value:n},e.children)}},8614:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var r=t(4848),o=t(8453);const i={title:"FAQ"},s=void 0,a={id:"faq",title:"FAQ",description:"How do I ensure preferred nodes are correctly labeled for use in my workspace?",source:"@site/docs/faq.md",sourceDirName:".",slug:"/faq",permalink:"/kaito/docs/faq",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/faq.md",tags:[],version:"current",frontMatter:{title:"FAQ"},sidebar:"sidebar",previous:{title:"Usage",permalink:"/kaito/docs/usage"},next:{title:"Inference",permalink:"/kaito/docs/inference"}},l={},c=[{value:"How do I ensure preferred nodes are correctly labeled for use in my workspace?",id:"how-do-i-ensure-preferred-nodes-are-correctly-labeled-for-use-in-my-workspace",level:3},{value:"How to upgrade the existing deployment to use the latest model configuration?",id:"how-to-upgrade-the-existing-deployment-to-use-the-latest-model-configuration",level:3},{value:"How to update model/inference parameters to override the KAITO Preset Configuration?",id:"how-to-update-modelinference-parameters-to-override-the-kaito-preset-configuration",level:3},{value:"What is the difference between instruct and non-instruct models?",id:"what-is-the-difference-between-instruct-and-non-instruct-models",level:3}];function d(e){const n={code:"code",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h3,{id:"how-do-i-ensure-preferred-nodes-are-correctly-labeled-for-use-in-my-workspace",children:"How do I ensure preferred nodes are correctly labeled for use in my workspace?"}),"\n",(0,r.jsx)(n.p,{children:"For using preferred nodes, make sure the node has the label specified in the labelSelector under matchLabels. For example, if your labelSelector is:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"labelSelector:\n  matchLabels:\n    apps: falcon-7b\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then the node should have the label: ",(0,r.jsx)(n.code,{children:"apps=falcon-7b"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"how-to-upgrade-the-existing-deployment-to-use-the-latest-model-configuration",children:"How to upgrade the existing deployment to use the latest model configuration?"}),"\n",(0,r.jsxs)(n.p,{children:["When using hosted public models, you can delete the existing inference workload (",(0,r.jsx)(n.code,{children:"Deployment"})," or ",(0,r.jsx)(n.code,{children:"StatefulSet"}),") manually, and the workspace controller will create a new one with the latest preset configuration (e.g., the image version) defined in the current release."]}),"\n",(0,r.jsx)(n.p,{children:"For private models, it is recommended to create a new workspace with a new image version in the Spec."}),"\n",(0,r.jsx)(n.h3,{id:"how-to-update-modelinference-parameters-to-override-the-kaito-preset-configuration",children:"How to update model/inference parameters to override the KAITO Preset Configuration?"}),"\n",(0,r.jsxs)(n.p,{children:["KAITO provides a limited capability to override preset configurations for models that use ",(0,r.jsx)(n.code,{children:"transformer"})," runtime manually."]}),"\n",(0,r.jsxs)(n.p,{children:["To update parameters for a deployed model, perform ",(0,r.jsx)(n.code,{children:"kubectl edit"})," against the workload, which could be either a ",(0,r.jsx)(n.code,{children:"StatefulSet"})," or ",(0,r.jsx)(n.code,{children:"Deployment"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["For example, to enable 4-bit quantization on a ",(0,r.jsx)(n.code,{children:"falcon-7b-instruct"})," deployment:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl edit deployment workspace-falcon-7b-instruct\n"})}),"\n",(0,r.jsx)(n.p,{children:"Within the deployment specification, locate and modify the command field."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Original:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"accelerate launch --num_processes 1 --num_machines 1 --machine_rank 0 --gpu_ids all inference_api.py --pipeline text-generation --torch_dtype bfloat16\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Modified to enable 4-bit Quantization:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"accelerate launch --num_processes 1 --num_machines 1 --machine_rank 0 --gpu_ids all inference_api.py --pipeline text-generation --torch_dtype bfloat16 --load_in_4bit\n"})}),"\n",(0,r.jsx)(n.p,{children:"Currently, we allow users to change the following parameters manually:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"pipeline"}),": For text-generation models this can be either ",(0,r.jsx)(n.code,{children:"text-generation"})," or ",(0,r.jsx)(n.code,{children:"conversational"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"load_in_4bit"})," or ",(0,r.jsx)(n.code,{children:"load_in_8bit"}),": Model quantization resolution."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Should you need to customize other parameters, kindly file an issue for potential future inclusion."}),"\n",(0,r.jsx)(n.h3,{id:"what-is-the-difference-between-instruct-and-non-instruct-models",children:"What is the difference between instruct and non-instruct models?"}),"\n",(0,r.jsx)(n.p,{children:"The main distinction lies in their intended use cases:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Instruct models"}),": Fine-tuned versions optimized for interactive chat applications. They are typically the preferred choice for most implementations due to their enhanced performance in conversational contexts."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Non-instruct (raw) models"}),": Designed for further fine-tuning with your own data."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);