"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[6480],{13779:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/function-calling-6642ad08e3422b9148c487beda51fc08.gif"},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(96540);const i={},l=o.createContext(i);function a(e){const n=o.useContext(l);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(l.Provider,{value:n},e.children)}},70672:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/mcp-bba18a9f0fb7a7b1ba5b2f419e93f49b.gif"},90754:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"tool-calling","title":"Tool Calling","description":"KAITO supports tool calling, allowing you to integrate external tools into your inference service. This feature enables the model to call APIs or execute functions based on the input it receives, enhancing its capabilities beyond text generation.","source":"@site/docs/tool-calling.md","sourceDirName":".","slug":"/tool-calling","permalink":"/kaito/docs/next/tool-calling","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/docs/tool-calling.md","tags":[],"version":"current","frontMatter":{"title":"Tool Calling"},"sidebar":"sidebar","previous":{"title":"Custom Model Integration","permalink":"/kaito/docs/next/custom-model"},"next":{"title":"Model As OCI Artifacts","permalink":"/kaito/docs/next/model-as-oci-artifacts"}}');var i=t(74848),l=t(28453);const a={title:"Tool Calling"},s=void 0,r={},c=[{value:"Requirements",id:"requirements",level:2},{value:"Supported Inference Runtimes",id:"supported-inference-runtimes",level:3},{value:"Supported Models",id:"supported-models",level:3},{value:"Examples",id:"examples",level:2},{value:"Named Function Calling",id:"named-function-calling",level:3},{value:"Model Context Protocol (MCP)",id:"model-context-protocol-mcp",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",section:"section",sup:"sup",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"KAITO supports tool calling, allowing you to integrate external tools into your inference service. This feature enables the model to call APIs or execute functions based on the input it receives, enhancing its capabilities beyond text generation."}),"\n",(0,i.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"supported-inference-runtimes",children:"Supported Inference Runtimes"}),"\n",(0,i.jsx)(n.p,{children:"Currently, tool calling is only supported with the vLLM inference runtime."}),"\n",(0,i.jsx)(n.h3,{id:"supported-models",children:"Supported Models"}),"\n",(0,i.jsx)(n.p,{children:"The following preset models are configured to support tool calling:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"phi-4-mini-instruct"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"phi-4"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"llama-3.1-8b-instruct"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"llama-3.3-70b-instruct"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"mistral-7b"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"mistral-7b-instruct"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"qwen2.5-coder-7b-instruct"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"qwen2.5-coder-32b-instruct"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"deepseek-r1-0528"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"deepseek-v3-0324"})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For more details on the inference configuration, refer to ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/features/tool_calling.html",children:"vLLM tool calling documentation"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsxs)(n.p,{children:["Assuming you have a running Workspace instance with the ",(0,i.jsx)(n.code,{children:"phi-4-mini-tool-call"})," preset model, you can use the following examples to test tool calling. First, ensure that the inference service is running and accessible:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/workspace-phi-4-mini-tool-call 8000\n"})}),"\n",(0,i.jsx)(n.h3,{id:"named-function-calling",children:"Named Function Calling"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Named Function Calling",src:t(13779).A+"",width:"1030",height:"428"}),"\n",(0,i.jsxs)(n.em,{children:["Source: Daily Dose of Data Science ",(0,i.jsx)(n.sup,{children:(0,i.jsx)(n.a,{href:"#user-content-fn-1",id:"user-content-fnref-1","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})})]})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")\n\ndef get_weather(location: str, unit: str):\n    return f"Getting the weather for {location} in {unit}..."\ntool_functions = {"get_weather": get_weather}\n\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "get_weather",\n        "description": "Get the current weather in a given location",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "location": {"type": "string", "description": "City and state, e.g., \'San Francisco, CA\'"},\n                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}\n            },\n            "required": ["location", "unit"]\n        }\n    }\n}]\n\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{"role": "user", "content": "What\'s the weather like in San Francisco?"}],\n    tools=tools,\n    tool_choice="auto"\n)\n\ntool_call = response.choices[0].message.tool_calls[0].function\nprint(f"Function called: {tool_call.name}")\nprint(f"Arguments: {tool_call.arguments}")\nprint(f"Result: {tool_functions[tool_call.name](**json.loads(tool_call.arguments))}")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Function called: get_weather\nArguments: {"location": "San Francisco, CA", "unit": "fahrenheit"}\nResult: Getting the weather for San Francisco, CA in fahrenheit...\n'})}),"\n",(0,i.jsx)(n.h3,{id:"model-context-protocol-mcp",children:"Model Context Protocol (MCP)"}),"\n",(0,i.jsxs)(n.p,{children:["With the right client framework, inference workload provisioned by KAITO can also call external tools using the ",(0,i.jsx)(n.a,{href:"https://modelcontextprotocol.io/",children:"Model Context Protocol (MCP)"}),". This allows the model to integrate and share data with external tools, systems, and data sources."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"MCP Overview",src:t(70672).A+"",width:"986",height:"568"}),"\n",(0,i.jsxs)(n.em,{children:["Source: Daily Dose of Data Science ",(0,i.jsx)(n.sup,{children:(0,i.jsx)(n.a,{href:"#user-content-fn-1",id:"user-content-fnref-1-2","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})})]})]}),"\n",(0,i.jsxs)(n.p,{children:["In the following example, we will use ",(0,i.jsx)(n.a,{href:"https://docs.astral.sh/uv/",children:"uv"})," to create a Python virtual environment and install the necessary dependencies for ",(0,i.jsx)(n.a,{href:"https://microsoft.github.io/autogen/stable//index.html",children:"AutoGen"})," to call the ",(0,i.jsx)(n.a,{href:"https://deepwiki.com/",children:"DeepWiki"})," MCP service and ask questions about the KAITO project."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'mkdir kaito-mcp\ncd kaito-mcp\n# Create and activate a virtual environment\nuv venv && source .venv/bin/activate\n# Install dependencies\nuv pip install "autogen-ext[openai]" "autogen-agentchat" "autogen-ext[mcp]"\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Create a Python script ",(0,i.jsx)(n.code,{children:"test.py"})," with the following content:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_core.models import ModelFamily, ModelInfo\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import (StreamableHttpMcpToolAdapter,\n                                   StreamableHttpServerParams)\nfrom openai import OpenAI\n\n\nasync def main() -> None:\n    # Create server params for the remote MCP service\n    server_params = StreamableHttpServerParams(\n        url="https://mcp.deepwiki.com/mcp",\n        timeout=30.0,\n        terminate_on_close=True,\n    )\n\n    # Get the ask_question tool from the server\n    adapter = await StreamableHttpMcpToolAdapter.from_server_params(server_params, "ask_question")\n\n    model = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy").models.list().data[0].id\n    model_info: ModelInfo = {\n        "vision": False,\n        "function_calling": True,\n        "json_output": True,\n        "family": ModelFamily.UNKNOWN,\n        "structured_output": True,\n        "multiple_system_messages": True,\n    }\n\n    # Create an agent that can use the ask_question tool\n    model_client = OpenAIChatCompletionClient(base_url="http://localhost:8000/v1", api_key="dummy", model=model, model_info=model_info)\n    agent = AssistantAgent(\n        name="deepwiki",\n        model_client=model_client,\n        tools=[adapter],\n        system_message="You are a helpful assistant.",\n    )\n\n    await Console(\n        agent.run_stream(task="In the GitHub repository \'kaito-project/kaito\', how many preset models are there?", cancellation_token=CancellationToken())\n    )\n\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,i.jsx)(n.p,{children:"To run the script, execute the following command in your terminal:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"uv run test.py\n"})}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'---------- TextMessage (user) ----------\nIn the GitHub repository \'kaito-project/kaito\', how many preset models are there?\n\n---------- ToolCallRequestEvent (deepwiki) ----------\n[FunctionCall(id=\'chatcmpl-tool-4e22b15c32d34430b80078a3acc41f0d\', arguments=\'{"repoName": "kaito-project/kaito", "question": "How many preset models are there?"}\', name=\'ask_question\')]\n\n---------- ToolCallExecutionEvent (deepwiki) ----------\n[FunctionExecutionResult(content=\'[{"type": "text", "text": "There are 16 preset models in the Kaito project.  These models are defined in the `supported_models.yaml` file  and registered programmatically within the codebase. ...", "annotations": null, "meta": null}]\', name=\'ask_question\', call_id=\'chatcmpl-tool-4e22b15c32d34430b80078a3acc41f0d\', is_error=False)]\n\n---------- ToolCallSummaryMessage (deepwiki) ----------\n[{"type": "text", "text": "There are 16 preset models in the Kaito project.  These models are defined in the `supported_models.yaml` file  and registered programmatically within the codebase. ...", "annotations": null, "meta": null}]\n'})}),"\n","\n",(0,i.jsxs)(n.section,{"data-footnotes":!0,className:"footnotes",children:[(0,i.jsx)(n.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{id:"user-content-fn-1",children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.dailydoseofds.com/p/function-calling-mcp-for-llms/",children:"https://www.dailydoseofds.com/p/function-calling-mcp-for-llms/"})," ",(0,i.jsx)(n.a,{href:"#user-content-fnref-1","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})," ",(0,i.jsxs)(n.a,{href:"#user-content-fnref-1-2","data-footnote-backref":"","aria-label":"Back to reference 1-2",className:"data-footnote-backref",children:["\u21a9",(0,i.jsx)(n.sup,{children:"2"})]})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);