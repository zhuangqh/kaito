"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[7565],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(96540);const i={},d=s.createContext(i);function o(e){const n=s.useContext(d);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(d.Provider,{value:n},e.children)}},33461:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"rag-api","title":"API Definitions and Examples","description":"A RAGEngine index is a logical collection that organizes and stores your documents for retrieval-augmented generation workflows. The relationship between indexes, documents, and document nodes is as follows:","source":"@site/versioned_docs/version-v0.7.x/rag-api.md","sourceDirName":".","slug":"/rag-api","permalink":"/kaito/docs/rag-api","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/versioned_docs/version-v0.7.x/rag-api.md","tags":[],"version":"v0.7.x","frontMatter":{"title":"API Definitions and Examples"},"sidebar":"sidebar","previous":{"title":"Retrieval-Augmented Generation (RAG)","permalink":"/kaito/docs/rag"},"next":{"title":"Custom Model Integration","permalink":"/kaito/docs/custom-model"}}');var i=t(74848),d=t(28453);const o={title:"API Definitions and Examples"},a=void 0,r={},l=[{value:"Creating an Index With Documents",id:"creating-an-index-with-documents",level:2},{value:"Create Index Request",id:"create-index-request",level:3},{value:"Create Index Response",id:"create-index-response",level:3},{value:"Splitting Documents with CodeSplitter",id:"splitting-documents-with-codesplitter",level:3},{value:"List Documents",id:"list-documents",level:2},{value:"List Documents Request",id:"list-documents-request",level:3},{value:"List Documents Response",id:"list-documents-response",level:3},{value:"Updating Documents",id:"updating-documents",level:2},{value:"Update Documents Request",id:"update-documents-request",level:3},{value:"Update Documents Response",id:"update-documents-response",level:3},{value:"Delete Documents",id:"delete-documents",level:2},{value:"Delete Documents Request",id:"delete-documents-request",level:3},{value:"Delete Documents Response",id:"delete-documents-response",level:3},{value:"Persist Index",id:"persist-index",level:2},{value:"Persist Index Request",id:"persist-index-request",level:3},{value:"Persist Index Response",id:"persist-index-response",level:3},{value:"Load Index",id:"load-index",level:2},{value:"Load Index Request",id:"load-index-request",level:3},{value:"Load Index Response",id:"load-index-response",level:3},{value:"Delete Index",id:"delete-index",level:2},{value:"Delete Index Request",id:"delete-index-request",level:3},{value:"Delete Index Response",id:"delete-index-response",level:3},{value:"Query Index",id:"query-index",level:2},{value:"Query Index Request",id:"query-index-request",level:3},{value:"Query Index Response",id:"query-index-response",level:3},{value:"OpenAI-Compatible Chat Completions",id:"openai-compatible-chat-completions",level:2},{value:"RAG Bypass Conditions",id:"rag-bypass-conditions",level:3},{value:"Chat Completions Request",id:"chat-completions-request",level:3},{value:"Chat Completions Response",id:"chat-completions-response",level:3},{value:"Example Python Client",id:"example-python-client",level:2}];function c(e){const n={a:"a",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["A ",(0,i.jsx)(n.strong,{children:"RAGEngine index"})," is a logical collection that organizes and stores your documents for retrieval-augmented generation workflows. The relationship between indexes, documents, and document nodes is as follows:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Index"}),": An index is a named container that holds a set of documents. Each index is independent and can be created, updated, queried, persisted, loaded, or deleted via the API."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Documents"}),": Documents are the primary units of content that you add to an index. Each document contains a ",(0,i.jsx)(n.code,{children:"text"})," field (the content to be indexed) and optional ",(0,i.jsx)(n.code,{children:"metadata"})," (such as author, source, or custom tags). When you add documents to an index, each is assigned a unique ",(0,i.jsx)(n.code,{children:"doc_id"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Document Nodes"}),": When a document is ingested, it is automatically split into smaller chunks called ",(0,i.jsx)(n.em,{children:"nodes"}),". The splitting strategy depends on the document type and metadata:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"By default, documents are split into sentences."}),"\n",(0,i.jsxs)(n.li,{children:["If you specify code-aware splitting (using the ",(0,i.jsx)(n.code,{children:"split_type"})," and ",(0,i.jsx)(n.code,{children:"language"})," metadata), the document is split into code blocks or logical code units."]}),"\n",(0,i.jsx)(n.li,{children:"Each node represents a chunk of text that is indexed and can be retrieved as part of a query."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How it works in practice:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"When you index a document, it is divided into nodes for efficient retrieval and semantic search."}),"\n",(0,i.jsx)(n.li,{children:"When you query an index, the engine retrieves the most relevant nodes (not necessarily whole documents) and can use them to generate answers or summaries."}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"source_nodes"})," field in query responses contains the actual nodes that matched your query, along with their scores and metadata."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This design enables fine-grained retrieval and more accurate, context-aware responses from your LLM-powered applications."}),"\n",(0,i.jsx)(n.h2,{id:"creating-an-index-with-documents",children:"Creating an Index With Documents"}),"\n",(0,i.jsxs)(n.p,{children:["To add documents to an index or create a new index, use the ",(0,i.jsx)(n.code,{children:"/index"})," API route. This endpoint accepts a POST request with the index name and a list of documents to be indexed."]}),"\n",(0,i.jsx)(n.h3,{id:"create-index-request",children:"Create Index Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'POST /index\n{\n  "index_name": "rag_index",\n  "documents": [\n    {\n      "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data.",\n      "metadata": {\n        "author": "kaito",\n      }\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index_name"}),": The name of the index to create or update."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"documents"}),": A list of documents, each with a ",(0,i.jsx)(n.code,{children:"text"})," field and optional ",(0,i.jsx)(n.code,{children:"metadata"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"create-index-response",children:"Create Index Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "doc_id": "123456",\n    "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data.",\n    "hash_value": "text_hash_value",\n    "metadata": {\n      "author": "kaito",\n    },\n    "is_truncated": false\n  }\n]\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Each returned document includes its unique ",(0,i.jsx)(n.code,{children:"doc_id"}),", the original text, metadata, and a flag indicating if the text was truncated. The ",(0,i.jsx)(n.code,{children:"doc_id"})," will be important for document update/delete calls."]}),"\n",(0,i.jsx)(n.h3,{id:"splitting-documents-with-codesplitter",children:"Splitting Documents with CodeSplitter"}),"\n",(0,i.jsxs)(n.p,{children:["By default, RAGEngine splits documents into sentences. However, you can instruct the engine to split documents using the ",(0,i.jsx)(n.code,{children:"CodeSplitter"})," (for code-aware chunking) by providing metadata in your API request."]}),"\n",(0,i.jsxs)(n.p,{children:["To use the ",(0,i.jsx)(n.code,{children:"CodeSplitter"}),", set the ",(0,i.jsx)(n.code,{children:"split_type"})," to ",(0,i.jsx)(n.code,{children:'"code"'})," and specify the programming language in the ",(0,i.jsx)(n.code,{children:"language"})," field of the document metadata. For example, when calling the RAGEngine API to index documents:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "documents": [\n    {\n      "text": "def foo():\\n    return 42\\n\\n# Another function\\ndef bar():\\n    pass",\n      "metadata": {\n        "split_type": "code",\n        "language": "python"\n      }\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["This instructs the RAGEngine to use code-aware splitting for the provided document. If ",(0,i.jsx)(n.code,{children:"split_type"})," is not set or set to any other value, sentence splitting will be used by default."]}),"\n",(0,i.jsx)(n.h2,{id:"list-documents",children:"List Documents"}),"\n",(0,i.jsxs)(n.p,{children:["To retrieve a paginated list of documents from a specific index, use the ",(0,i.jsx)(n.code,{children:"/indexes/{index_name}/documents"})," API route. This endpoint accepts a GET request with optional query parameters for pagination, text truncation, and metadata filtering."]}),"\n",(0,i.jsx)(n.h3,{id:"list-documents-request",children:"List Documents Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"GET /indexes/rag_index/documents?limit=5&offset=0&max_text_length=500\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"limit"}),": (optional) Maximum number of documents to return (default: 10, max: 100)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"offset"}),": (optional) Starting point for the document list (default: 0)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_text_length"}),": (optional) Maximum text length to return per document (default: 1000)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"metadata_filter"}),": (optional) A JSON string representing key-value pairs to filter documents by their metadata."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"list-documents-response",children:"List Documents Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "documents": [\n    {\n      "doc_id": "123456",\n      "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data.",\n      "hash_value": "text_hash_value",\n      "metadata": {\n        "author": "kaito",\n      },\n      "is_truncated": false\n    }\n  ],\n  "count": 1\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Each document in the response includes its unique ",(0,i.jsx)(n.code,{children:"doc_id"}),", the (possibly truncated) text, metadata, and a flag indicating if the text was truncated."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"}),(0,i.jsx)(n.br,{}),"\n","If you want to filter documents by metadata, provide the ",(0,i.jsx)(n.code,{children:"metadata_filter"})," parameter as a JSON string. For example:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'GET /indexes/rag_index/documents?metadata_filter={"author":"kaito"}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"updating-documents",children:"Updating Documents"}),"\n",(0,i.jsxs)(n.p,{children:["To update existing documents in a specific index, use the ",(0,i.jsx)(n.code,{children:"/indexes/{index_name}/documents"})," API route. This endpoint accepts a POST request with the index name in the URL and a list of documents to update in the request body."]}),"\n",(0,i.jsx)(n.h3,{id:"update-documents-request",children:"Update Documents Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'POST /indexes/rag_index/documents\n{\n  "documents": [\n    {\n      "doc_id": "123456",\n      "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data. Adding an information retrieval system gives you control over grounding data used by an LLM when it formulates a response.",\n      "hash_value": "text_hash_value",\n      "metadata": {\n        "author": "kaito",\n      }\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"doc_id"}),": The unique identifier of the document to update."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"text"}),": The new or updated text for the document."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"metadata"}),": (Optional) Updated metadata for the document."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"update-documents-response",children:"Update Documents Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "updated_documents": [\n    {\n      "doc_id": "123456",\n      "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data. Adding an information retrieval system gives you control over grounding data used by an LLM when it formulates a response.",\n      "hash_value": "text_hash_value",\n      "metadata": {\n        "author": "kaito",\n      }\n    }\n  ],\n  "unchanged_documents": [],\n  "not_found_documents": []\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"updated_documents"}),": Documents that were successfully updated."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"unchanged_documents"}),": Documents that were provided but did not require changes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"not_found_documents"}),": Documents with IDs that were not found in the index."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to keep your indexed documents up to date with the latest content or metadata."}),"\n",(0,i.jsx)(n.h2,{id:"delete-documents",children:"Delete Documents"}),"\n",(0,i.jsxs)(n.p,{children:["To delete one or more documents from a specific index, use the ",(0,i.jsx)(n.code,{children:"/indexes/{index_name}/documents/delete"})," API route. This endpoint accepts a POST request with the index name in the URL and a list of document IDs to delete in the request body."]}),"\n",(0,i.jsx)(n.h3,{id:"delete-documents-request",children:"Delete Documents Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'POST /indexes/rag_index/documents/delete\n{\n  "doc_ids": ["123456"]\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"doc_ids"}),": A list of document IDs to delete from the specified index."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"delete-documents-response",children:"Delete Documents Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "deleted_doc_ids": ["123456"],\n  "not_found_doc_ids": []\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"deleted_doc_ids"}),": Document IDs that were successfully deleted."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"not_found_doc_ids"}),": Document IDs that were not found in the index."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to remove documents that are no longer needed from your index."}),"\n",(0,i.jsx)(n.h2,{id:"persist-index",children:"Persist Index"}),"\n",(0,i.jsxs)(n.p,{children:["To save (persist) the data of an index to disk, use the ",(0,i.jsx)(n.code,{children:"/persist/{index_name}"})," API route. This endpoint accepts a POST request with the index name in the URL and an optional ",(0,i.jsx)(n.code,{children:"path"})," query parameter specifying where to save the index data."]}),"\n",(0,i.jsx)(n.h3,{id:"persist-index-request",children:"Persist Index Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"POST /persist/rag_index?path=./custom_path\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index_name"}),": The name of the index to persist."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"path"}),": (optional) The directory path where the index will be saved. If not provided, the default directory is used."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"persist-index-response",children:"Persist Index Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "message": "Successfully persisted index rag_index to ./custom_path/rag_index."\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to ensure your indexed data is safely stored on disk."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"load-index",children:"Load Index"}),"\n",(0,i.jsxs)(n.p,{children:["To load an existing index from disk, use the ",(0,i.jsx)(n.code,{children:"/load/{index_name}"})," API route. This endpoint accepts a POST request with the index name in the URL, an optional ",(0,i.jsx)(n.code,{children:"path"})," query parameter specifying where to load the index from, and an optional ",(0,i.jsx)(n.code,{children:"overwrite"})," flag."]}),"\n",(0,i.jsx)(n.h3,{id:"load-index-request",children:"Load Index Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"POST /load/rag_index?path=./custom_path/rag_index\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index_name"}),": The name of the index to load."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"path"}),": (optional) The path to load the index from. If not provided, the default directory is used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"overwrite"}),": (optional, default: false) If true, will overwrite the existing index if it already exists in memory."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"load-index-response",children:"Load Index Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "message": "Successfully loaded index rag_index from ./custom_path/rag_index."\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to restore previously persisted indexes into memory for querying and updates."}),"\n",(0,i.jsx)(n.h2,{id:"delete-index",children:"Delete Index"}),"\n",(0,i.jsxs)(n.p,{children:["To delete an entire index and all of its documents, use the ",(0,i.jsx)(n.code,{children:"/indexes/{index_name}"})," API route. This endpoint accepts a DELETE request with the index name in the URL. Deleting an index is irreversible and will remove all associated documents from memory."]}),"\n",(0,i.jsx)(n.h3,{id:"delete-index-request",children:"Delete Index Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"DELETE /indexes/rag_index\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index_name"}),": The name of the index to delete."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"delete-index-response",children:"Delete Index Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "message": "Successfully deleted index rag_index."\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to permanently remove an index and all its data when it is no longer needed."}),"\n",(0,i.jsx)(n.h2,{id:"query-index",children:"Query Index"}),"\n",(0,i.jsxs)(n.p,{children:["To query a specific index for relevant documents, use the ",(0,i.jsx)(n.code,{children:"/query"})," API route. This endpoint accepts a POST request with the index name, query string, and optional parameters for result count, and LLM generation."]}),"\n",(0,i.jsx)(n.h3,{id:"query-index-request",children:"Query Index Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'POST /query\n{\n  "index_name": "rag_index",\n  "query": "What is RAG?",\n  "top_k": 5,\n  "llm_params": {\n    "temperature": 0.7,\n    "max_tokens": 2048\n  }\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index_name"}),": The name of the index to query."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"query"}),": The query string."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"top_k"}),": (optional) Number of top documents to retrieve (default: 5)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"llm_params"}),": (optional) Parameters for LLM-based generation (e.g., temperature, max_tokens)."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"query-index-response",children:"Query Index Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "response": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model...",\n  "source_nodes": [\n    {\n      "doc_id": "123456",\n      "node_id": "2853a565-8c1f-4982-acaa-a0ab52691435",\n      "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model...",\n      "score": 0.95,\n      "metadata": {\n        "author": "kaito",\n      }\n    }\n  ],\n  "metadata": {\n    "2853a565-8c1f-4982-acaa-a0ab52691435": {\n      "author": "kaito",\n    }\n  }\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"response"}),": The generated answer or summary from the LLM (if enabled)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"source_nodes"}),": List of source nodes with their text, score, and metadata."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"metadata"}),": Additional metadata about the query or response."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to retrieve relevant information from your indexed documents and optionally generate answers using an LLM."}),"\n",(0,i.jsx)(n.h2,{id:"openai-compatible-chat-completions",children:"OpenAI-Compatible Chat Completions"}),"\n",(0,i.jsxs)(n.p,{children:["The RAGEngine provides an OpenAI-compatible chat completions endpoint at ",(0,i.jsx)(n.code,{children:"/v1/chat/completions"}),". This endpoint allows you to use RAG capabilities with the familiar OpenAI API format, making it easy to integrate with existing applications that use OpenAI's chat completions."]}),"\n",(0,i.jsx)(n.h3,{id:"rag-bypass-conditions",children:"RAG Bypass Conditions"}),"\n",(0,i.jsxs)(n.p,{children:["The chat completions endpoint automatically determines whether to use RAG or pass requests directly to the LLM based on the request content. Requests will ",(0,i.jsx)(n.strong,{children:"bypass RAG"})," and be sent directly to the LLM in the following cases:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"No Index Specified"}),": When ",(0,i.jsx)(n.code,{children:"index_name"})," is not provided in the request, the system treats it as a standard LLM request without retrieval."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tool/Function Calls"}),": When the request contains ",(0,i.jsx)(n.code,{children:"tools"})," or ",(0,i.jsx)(n.code,{children:"functions"})," parameters, indicating the use of function calling capabilities."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Unsupported Message Roles"}),": When messages contain roles other than ",(0,i.jsx)(n.code,{children:"user"}),", ",(0,i.jsx)(n.code,{children:"system"}),", ",(0,i.jsx)(n.code,{children:"assistant"}),", or ",(0,i.jsx)(n.code,{children:"developer"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Non-Text Content"}),": When user messages contain non-text content types (such as images, audio, or other media formats)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"When any of these conditions are met, the request is passed through directly to the underlying LLM without document retrieval or augmentation, ensuring full compatibility with standard OpenAI API usage patterns."}),"\n",(0,i.jsx)(n.h3,{id:"chat-completions-request",children:"Chat Completions Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'POST /v1/chat/completions\n{\n  "index_name": "rag_index",\n  "model": "example_model",\n  "messages": [\n    {"role": "system", "content": "You are a knowledgeable assistant."},\n    {"role": "user", "content": "What is RAG?"}\n  ],\n  "temperature": 0.7,\n  "max_tokens": 2048,\n  "context_token_ratio": 0.5\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index_name"}),": (optional) The name of the index to query for relevant documents. If not included, the request will be sent to the LLM directly with no additional context."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model"}),": The model identifier (for compatibility)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"messages"}),": Array of message objects with ",(0,i.jsx)(n.code,{children:"role"})," and ",(0,i.jsx)(n.code,{children:"content"})," fields."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"temperature"}),": (optional) Controls randomness in the response (0.0 to 1.0)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_tokens"}),": (optional) Maximum number of tokens to generate."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"context_token_ratio"}),": (optional) The percentage of tokens from the available context to add documents from the RAG. If the ",(0,i.jsx)(n.code,{children:"max_tokens"})," parameter is used, the amount of tokens filled with RAG documents will be ~= ",(0,i.jsx)(n.code,{children:"max_tokens * context_token_ratio"}),". If not, the amount of tokens filled with RAG documents will be ~= ",(0,i.jsx)(n.code,{children:"(llm_context_window_size - prompt_token_count) * context_token_ratio"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"chat-completions-response",children:"Chat Completions Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-123",\n  "object": "chat.completion",\n  "created": 1677652288,\n  "model": "example_model",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "RAG stands for Retrieval-Augmented Generation. It\'s an architecture that augments the capabilities of a Large Language Model by adding an information retrieval system that provides grounding data."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "source_nodes": [\n    {\n      "doc_id": "123456",\n      "node_id": "2853a565-8c1f-4982-acaa-a0ab52691435",\n      "text": "Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model...",\n      "score": 0.95,\n      "metadata": {\n        "author": "kaito",\n      }\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"id"}),": Unique identifier for the chat completion."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"object"}),": Type of object returned (",(0,i.jsx)(n.code,{children:"chat.completion"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"created"}),": Unix timestamp of when the completion was created."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model"}),": The model used for the completion."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"choices"}),": Array of completion choices (typically one)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"source_nodes"}),": (RAG-specific) List of source documents that informed the response."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Use this endpoint to integrate RAG capabilities into applications that already use OpenAI's chat completions API format."}),"\n",(0,i.jsx)(n.h2,{id:"example-python-client",children:"Example Python Client"}),"\n",(0,i.jsxs)(n.p,{children:["You can leverage the ",(0,i.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(40052).A+"",children:"example_rag_client.py"})," as a starting point for a rag client with inputs that match the route documentation above."]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},40052:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/files/example_rag_client-300d57a551ae76d3fdc98e69d2cdf037.py"}}]);