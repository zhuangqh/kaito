"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[6503],{28453:(e,t,s)=>{s.d(t,{R:()=>i,x:()=>c});var n=s(96540);const r={},d=n.createContext(r);function i(e){const t=n.useContext(d);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),n.createElement(d.Provider,{value:t},e.children)}},62037:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>c,default:()=>x,frontMatter:()=>i,metadata:()=>n,toc:()=>o});const n=JSON.parse('{"id":"monitoring","title":"Monitoring","description":"Inference service exposes Prometheus metrics for monitoring system stats.","source":"@site/docs/monitoring.md","sourceDirName":".","slug":"/monitoring","permalink":"/kaito/docs/next/monitoring","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/docs/monitoring.md","tags":[],"version":"current","frontMatter":{"title":"Monitoring"},"sidebar":"sidebar","previous":{"title":"Gateway API Inference Extension","permalink":"/kaito/docs/next/gateway-api-inference-extension"},"next":{"title":"OOM Prevention","permalink":"/kaito/docs/next/kaito-oom-prevention"}}');var r=s(74848),d=s(28453);const i={title:"Monitoring"},c=void 0,l={},o=[{value:"vLLM runtime",id:"vllm-runtime",level:2}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",p:"p",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,d.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:"Inference service exposes Prometheus metrics for monitoring system stats."}),"\n",(0,r.jsx)(t.h2,{id:"vllm-runtime",children:"vLLM runtime"}),"\n",(0,r.jsxs)(t.p,{children:["vLLM exposes Prometheus metrics at the ",(0,r.jsx)(t.code,{children:"/metrics"})," endpoint. These metrics provide detailed insights into the system's performance, resource utilization, and request processing statistics. The following table lists all available metrics:"]}),"\n",(0,r.jsx)(t.p,{children:"vLLM version: 0.8.2"}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["vLLM V1 engine is now enabled by default. Check ",(0,r.jsx)(t.a,{href:"https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html",children:"here"})," for details."]})}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Category"}),(0,r.jsx)(t.th,{children:"Metric Name"}),(0,r.jsx)(t.th,{children:"Type"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Scheduler"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:num_requests_running"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"Number of requests currently running on GPU"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Scheduler"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:num_requests_waiting"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"Number of requests waiting to be processed"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Scheduler"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:lora_requests_info"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"Running stats on lora requests"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Scheduler"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:num_requests_swapped"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"Number of requests swapped to CPU (DEPRECATED: KV cache offloading is not used in V1)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Cache"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:gpu_cache_usage_perc"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"GPU KV-cache usage. 1 means 100 percent usage"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Cache"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:cpu_cache_usage_perc"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"CPU KV-cache usage. 1 means 100 percent usage (DEPRECATED: KV cache offloading is not used in V1)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Cache"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:cpu_prefix_cache_hit_rate"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"CPU prefix cache block hit rate (DEPRECATED: KV cache offloading is not used in V1)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Cache"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:gpu_prefix_cache_hit_rate"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsxs)(t.td,{children:["GPU prefix cache block hit rate (DEPRECATED: use ",(0,r.jsx)(t.code,{children:"vllm:gpu_prefix_cache_queries"})," and ",(0,r.jsx)(t.code,{children:"vllm:gpu_prefix_cache_hits in V1"}),")"]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Cache"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:gpu_cache_usage_perc"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"GPU prefix cache queries, in terms of number of queried blocks (V1 only)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"System Stats - Cache"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:gpu_prefix_cache_hits"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"GPU prefix cache hits, in terms of number of cached blocks (V1 only)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Iteration Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:num_preemptions_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Cumulative number of preemption from the engine"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Iteration Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:prompt_tokens_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Number of prefill tokens processed"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Iteration Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:generation_tokens_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Number of generation tokens processed"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Iteration Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:iteration_tokens_total"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Number of tokens per engine_step"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Iteration Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:time_to_first_token_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Time to first token in seconds"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Iteration Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:time_per_output_token_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Time per output token in seconds"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:e2e_request_latency_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"End to end request latency in seconds"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_queue_time_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Time spent in WAITING phase for request"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_inference_time_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Time spent in RUNNING phase for request"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_prefill_time_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Time spent in PREFILL phase for request"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_decode_time_seconds"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Time spent in DECODE phase for request"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_prompt_tokens"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Number of prefill tokens processed per request"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_generation_tokens"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Number of generation tokens processed per request"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_max_num_generation_tokens"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"Maximum number of requested generation tokens"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_params_n"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"The 'n' request parameter"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_params_max_tokens"})}),(0,r.jsx)(t.td,{children:"Histogram"}),(0,r.jsx)(t.td,{children:"The 'max_tokens' request parameter"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Request Stats"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:request_success_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Count of successfully processed requests"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Speculative Decoding"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:spec_decode_draft_acceptance_rate"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"Speculative token acceptance rate (DEPRECATED: Unused in V1)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Speculative Decoding"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:spec_decode_efficiency"})}),(0,r.jsx)(t.td,{children:"Gauge"}),(0,r.jsx)(t.td,{children:"Speculative decoding system efficiency (DEPRECATED: Unused in V1)"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Speculative Decoding"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:spec_decode_num_accepted_tokens_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Number of accepted tokens"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Speculative Decoding"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:spec_decode_num_draft_tokens_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Number of draft tokens"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Speculative Decoding"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"vllm:spec_decode_num_emitted_tokens_total"})}),(0,r.jsx)(t.td,{children:"Counter"}),(0,r.jsx)(t.td,{children:"Number of emitted tokens (DEPRECATED: Unused in V1)"})]})]})]})]})}function x(e={}){const{wrapper:t}={...(0,d.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}}}]);