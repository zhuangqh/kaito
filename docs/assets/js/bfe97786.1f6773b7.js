"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[6729],{5818:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"multi-node-inference","title":"Multi-Node Inference","description":"This document explains how to configure and use multi-node distributed inference in KAITO for large models that require more GPU resources than a single node can provide.","source":"@site/versioned_docs/version-v0.6.x/multi-node-inference.md","sourceDirName":".","slug":"/multi-node-inference","permalink":"/kaito/docs/multi-node-inference","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/versioned_docs/version-v0.6.x/multi-node-inference.md","tags":[],"version":"v0.6.x","frontMatter":{"title":"Multi-Node Inference"},"sidebar":"sidebar","previous":{"title":"Inference","permalink":"/kaito/docs/inference"},"next":{"title":"Fine Tuning","permalink":"/kaito/docs/tuning"}}');var s=i(4848),l=i(8453);const t={title:"Multi-Node Inference"},o=void 0,d={},a=[{value:"Overview",id:"overview",level:2},{value:"When to Use Multi-Node Inference",id:"when-to-use-multi-node-inference",level:2},{value:"Supported Models",id:"supported-models",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Basic Multi-Node Setup",id:"basic-multi-node-setup",level:3},{value:"Pre-Provisioned Nodes",id:"pre-provisioned-nodes",level:3},{value:"Custom vLLM Parameters for Multi-Node",id:"custom-vllm-parameters-for-multi-node",level:3},{value:"Architecture",id:"architecture",level:2},{value:"Single-Node Multi-GPU",id:"single-node-multi-gpu",level:3},{value:"Multi-Node Multi-GPU",id:"multi-node-multi-gpu",level:3},{value:"Resource Validation",id:"resource-validation",level:2},{value:"Service Architecture",id:"service-architecture",level:2},{value:"Health Monitoring",id:"health-monitoring",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Service Unavailable After Deployment",id:"service-unavailable-after-deployment",level:3},{value:"Worker Pod Failures",id:"worker-pod-failures",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"API Usage",id:"api-usage",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Related Documentation",id:"related-documentation",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This document explains how to configure and use multi-node distributed inference in KAITO for large models that require more GPU resources than a single node can provide."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Multi-node inference allows you to deploy large AI models across multiple nodes (servers) when the model is too large to fit on a single node. KAITO supports different parallelism strategies depending on your model's requirements:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Strategy"}),(0,s.jsx)(n.th,{children:"Use Case"}),(0,s.jsx)(n.th,{children:"Supported"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Single GPU"})}),(0,s.jsx)(n.td,{children:"Small models that fit on one GPU"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Single-Node Multi-GPU"})}),(0,s.jsx)(n.td,{children:"Models that need multiple GPUs but fit on one node"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Multi-Node Multi-GPU"})}),(0,s.jsx)(n.td,{children:"Very large models (400B+ parameters) requiring multiple nodes"}),(0,s.jsx)(n.td,{children:"\u2705"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use-multi-node-inference",children:"When to Use Multi-Node Inference"}),"\n",(0,s.jsx)(n.p,{children:"Consider multi-node inference when:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Your model has 400B+ parameters and cannot fit on a single node"}),"\n",(0,s.jsx)(n.li,{children:"You need to serve models like very large language models that exceed single-node memory capacity"}),"\n",(0,s.jsx)(n.li,{children:"You have specific performance requirements that benefit from distributed processing"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Multi-node inference introduces additional complexity and network overhead. Only use it when your model truly requires more resources than a single node can provide."})}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,s.jsx)(n.p,{children:"The following preset models support multi-node distributed inference:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llama3"}),": ",(0,s.jsx)(n.code,{children:"llama-3.3-70b-instruct"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DeepSeek"}),": ",(0,s.jsx)(n.code,{children:"deepseek-r1-0528"}),", ",(0,s.jsx)(n.code,{children:"deepseek-v3-0324"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Check the ",(0,s.jsx)(n.a,{href:"/kaito/docs/presets",children:"presets documentation"})," for the complete list and their specific requirements."]}),"\n",(0,s.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"basic-multi-node-setup",children:"Basic Multi-Node Setup"}),"\n",(0,s.jsx)(n.p,{children:"To deploy a model across multiple nodes, specify the node count in your Workspace configuration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: kaito.sh/v1beta1\nkind: Workspace\nmetadata:\n  name: workspace-large-model\nresource:\n  count: 2                    # Number of nodes to use\n  instanceType: "Standard_NC80adis_H100_v5"\n  labelSelector:\n    matchLabels:\n      apps: large-model\ninference:\n  preset:\n    name: "llama-3.3-70b-instruct"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"pre-provisioned-nodes",children:"Pre-Provisioned Nodes"}),"\n",(0,s.jsx)(n.p,{children:"If you're using pre-provisioned GPU nodes, specify them explicitly:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: kaito.sh/v1beta1\nkind: Workspace\nmetadata:\n  name: workspace-large-model\nresource:\n  count: 2\n  preferredNodes:\n    - gpu-node-1\n    - gpu-node-2\n  labelSelector:\n    matchLabels:\n      apps: large-model\ninference:\n  preset:\n    name: "llama-3.3-70b-instruct"\n'})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["Pre-provisioned nodes must have the same matching labels as specified in the ",(0,s.jsx)(n.code,{children:"resource"})," spec, and each node must report available GPU resources."]})}),"\n",(0,s.jsx)(n.h3,{id:"custom-vllm-parameters-for-multi-node",children:"Custom vLLM Parameters for Multi-Node"}),"\n",(0,s.jsx)(n.p,{children:"You can customize vLLM runtime parameters for distributed inference using a ConfigMap:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: distributed-inference-config\ndata:\n  inference_config.yaml: |\n    vllm:\n      gpu-memory-utilization: 0.95\n      max-model-len: 131072\n---\napiVersion: kaito.sh/v1beta1\nkind: Workspace\nmetadata:\n  name: workspace-large-model\nresource:\n  count: 2\n  instanceType: "Standard_NC80adis_H100_v5"\n  labelSelector:\n    matchLabels:\n      apps: large-model\ninference:\n  preset:\n    name: "llama-3.3-70b-instruct"\n  config: "distributed-inference-config"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Key parameters for multi-node inference:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"tensor-parallel-size"}),": Automatically set by KAITO based on the number of GPUs per node"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"pipeline-parallel-size"}),": Automatically set by KAITO based on the number of nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gpu-memory-utilization"}),": Fraction of GPU memory to use (0.0-1.0) - user configurable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"max-model-len"}),": Maximum sequence length - user configurable"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"tensor-parallel-size"})," and ",(0,s.jsx)(n.code,{children:"pipeline-parallel-size"})," parameters are automatically managed by KAITO based on your cluster configuration and do not need to be specified in the ConfigMap."]})}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"single-node-multi-gpu",children:"Single-Node Multi-GPU"}),"\n",(0,s.jsxs)(n.p,{children:["When using multiple GPUs on a single node, KAITO uses ",(0,s.jsx)(n.strong,{children:"tensor parallelism"})," to split the model across GPUs within that node:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Node 1                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502GPU 1\u2502 \u2502GPU 2\u2502 \u2502GPU 3\u2502 \u2502GPU 4\u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502           Model Split               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multi-node-multi-gpu",children:"Multi-Node Multi-GPU"}),"\n",(0,s.jsxs)(n.p,{children:["For multi-node deployments, KAITO combines ",(0,s.jsx)(n.strong,{children:"pipeline parallelism"})," between nodes and ",(0,s.jsx)(n.strong,{children:"tensor parallelism"})," within each node:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       Node 1        \u2502    \u2502       Node 2        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2510     \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502GPU 1\u2502\u2502GPU 2\u2502     \u2502\u25c4\u2500\u2500\u25ba\u2502  \u2502GPU 3\u2502 \u2502GPU 4\u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2518     \u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502   Layers 1-N/2      \u2502    \u2502   Layers N/2+1-N    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"resource-validation",children:"Resource Validation"}),"\n",(0,s.jsx)(n.p,{children:"KAITO automatically validates that your configuration provides sufficient resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Count"}),": ",(0,s.jsx)(n.code,{children:"(GPUs per instance) \xd7 (workspace.resource.count) \u2265 (Required GPUs for model)"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": ",(0,s.jsx)(n.code,{children:"(GPU memory) \xd7 (Total GPUs) \u2265 (Required model memory)"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If validation fails, you'll receive an error message when creating or updating the workspace."}),"\n",(0,s.jsx)(n.admonition,{title:"Resource Optimization",type:"tip",children:(0,s.jsxs)(n.p,{children:["KAITO may use fewer nodes than specified in ",(0,s.jsx)(n.code,{children:"workspace.resource.count"})," if the model can fit efficiently on fewer nodes. This optimizes GPU utilization and reduces network overhead, but be mindful of the costs when provisioning many nodes."]})}),"\n",(0,s.jsx)(n.h2,{id:"service-architecture",children:"Service Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Multi-node inference uses Kubernetes StatefulSets to ensure stable pod identity and coordination:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Leader Pod"})," (index 0): Coordinates the distributed inference and serves the API"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Worker Pods"})," (index 1+): Join the Ray cluster and participate in model serving"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The service endpoint points to the leader pod, which handles all incoming requests and coordinates with worker pods."}),"\n",(0,s.jsx)(n.admonition,{title:"Future Enhancement",type:"note",children:(0,s.jsxs)(n.p,{children:["KAITO will support ",(0,s.jsx)(n.a,{href:"https://lws.sigs.k8s.io/docs/overview/",children:"LeaderWorkerSet"})," in the future to provide better management of leader-worker topologies and improved fault tolerance for multi-node deployments."]})}),"\n",(0,s.jsx)(n.h2,{id:"health-monitoring",children:"Health Monitoring"}),"\n",(0,s.jsx)(n.p,{children:"Multi-node deployments use specialized health checks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Liveness Probes"}),": Monitor Ray cluster health and detect dead actors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Readiness Probes"}),": Check service availability via the leader pod's ",(0,s.jsx)(n.code,{children:"/health"})," endpoint"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If worker pods fail, the leader will restart to reinitialize the entire cluster, ensuring all pods are synchronized."}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Planning"}),": Carefully plan your GPU and memory requirements before deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network Bandwidth"}),": Ensure sufficient network bandwidth between nodes for optimal performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Monitor both individual node health and overall cluster performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Management"}),": Be aware that multi-node deployments can be expensive; only use when necessary"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"service-unavailable-after-deployment",children:"Service Unavailable After Deployment"}),"\n",(0,s.jsx)(n.p,{children:"If the service becomes unavailable:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Check if all pods are running: ",(0,s.jsx)(n.code,{children:"kubectl get pods -l app=<your-app-label>"})]}),"\n",(0,s.jsx)(n.li,{children:"Verify Ray cluster health in leader pod logs"}),"\n",(0,s.jsx)(n.li,{children:"Ensure network connectivity between nodes"}),"\n",(0,s.jsx)(n.li,{children:"Check resource allocation and GPU availability"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"worker-pod-failures",children:"Worker Pod Failures"}),"\n",(0,s.jsx)(n.p,{children:"Worker pod failures will trigger leader pod restart to reinitialize the cluster:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor pod restart events"}),"\n",(0,s.jsx)(n.li,{children:"Check for resource constraints (memory, GPU)"}),"\n",(0,s.jsx)(n.li,{children:"Verify node-to-node network connectivity"}),"\n",(0,s.jsx)(n.li,{children:"Review pod logs for Ray cluster connection issues"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,s.jsx)(n.p,{children:"If you experience poor performance:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor network latency between nodes"}),"\n",(0,s.jsx)(n.li,{children:"Check GPU utilization across all nodes"}),"\n",(0,s.jsx)(n.li,{children:"Review memory usage and potential bottlenecks"}),"\n",(0,s.jsx)(n.li,{children:"Consider adjusting parallelism parameters"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"api-usage",children:"API Usage"}),"\n",(0,s.jsx)(n.p,{children:"Multi-node inference services expose the same API as single-node deployments. The vLLM runtime provides OpenAI-compatible endpoints:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Get the cluster IP of your service\nkubectl get services\n\n# Check service health\nkubectl run -it --rm --restart=Never curl --image=curlimages/curl -- \\\n  curl http://<CLUSTER-IP>:80/health\n\n# Generate text using chat completions API\nkubectl run -it --rm --restart=Never curl --image=curlimages/curl -- \\\n  curl -X POST http://<CLUSTER-IP>:80/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "llama-3.3-70b-instruct",\n    "messages": [{"role": "user", "content": "Your prompt here"}],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsxs)(n.p,{children:["For detailed API specifications, see the ",(0,s.jsx)(n.a,{href:"/kaito/docs/inference#inference-api",children:"inference documentation"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom Models"}),": Multi-node inference is currently only supported for preset models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fault Tolerance"}),": The system requires leader restart when worker pods fail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network Dependency"}),": Performance heavily depends on inter-node network quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complexity"}),": Debugging and monitoring are more complex than single-node deployments"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/kaito/docs/inference",children:"Inference"})," - General inference documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/kaito/docs/presets",children:"Presets"})," - Supported models and their requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/kaito/docs/custom-model",children:"Custom Model"})," - Using custom models (single-node only)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);