"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[749],{5196:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/llama-3.3-70b-instruct-ba868b67fbcfd93cc2be4f136d87d75f.png"},8103:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>o});var n=s(4848),r=s(8453);const i={title:"Proposal for new model support",authors:["@chewong"],reviewers:["KAITO contributor"],"creation-date":new Date("2025-05-29T00:00:00.000Z"),"last-updated":new Date("2025-05-29T00:00:00.000Z"),status:"provisional"},a="Title",l={id:"proposals/llama-3.3-70b-instruct",title:"Proposal for new model support",description:"Add meta-llama/Llama-3.3-70B-Instruct to KAITO supported model list",source:"@site/docs/proposals/20250529-llama-3.3-70b-instruct.md",sourceDirName:"proposals",slug:"/proposals/llama-3.3-70b-instruct",permalink:"/kaito/docs/proposals/llama-3.3-70b-instruct",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250529-llama-3.3-70b-instruct.md",tags:[],version:"current",sidebarPosition:20250529,frontMatter:{title:"Proposal for new model support",authors:["@chewong"],reviewers:["KAITO contributor"],"creation-date":"2025-05-29T00:00:00.000Z","last-updated":"2025-05-29T00:00:00.000Z",status:"provisional"}},c={},o=[{value:"Glossary",id:"glossary",level:2},{value:"Summary",id:"summary",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Runtimes",id:"runtimes",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"title",children:"Title"}),"\n",(0,n.jsxs)(t.p,{children:["Add ",(0,n.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",children:"meta-llama/Llama-3.3-70B-Instruct"})," to KAITO supported model list"]}),"\n",(0,n.jsx)(t.h2,{id:"glossary",children:"Glossary"}),"\n",(0,n.jsx)(t.p,{children:"N/A"}),"\n",(0,n.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Model description"}),": The Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Model usage statistics"}),": 700,643 downloads in the last 30 days as of 2025-05-29, according to ",(0,n.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",children:"Hugging Face"}),"."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Model license"}),": The model is licensed under the ",(0,n.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE",children:"LLAMA 3.3 COMMUNITY LICENSE"}),". Therefore, users must agree to the terms of the license and supply Hugging Face credentials to download the model files."]}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-yaml",children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: hf-token\n  namespace: default\ndata:\n  HF_TOKEN: <base64-encoded-huggingface-token>\ntype: Opaque\n"})}),"\n",(0,n.jsx)(t.p,{children:"In the Workspace configuration, you can specify the secret name to use the Hugging Face token:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-yaml",children:"apiVersion: kaito.sh/v1beta1\nkind: Workspace\nmetadata:\n  name: workspace-llama-3-3-70b-instruct\nresource:\n  instanceType: Standard_ND96isr_H100_v5\n  labelSelector:\n    matchLabels:\n      apps: llama-3-3-70b-instruct\ninference:\n  preset:\n    name: llama-3.3-70b-instruct\n    presetOptions:\n      modelAccessSecret: hf-token\n"})}),"\n",(0,n.jsx)(t.h2,{id:"requirements",children:"Requirements"}),"\n",(0,n.jsx)(t.p,{children:"The following table describes the basic model characteristics and the resource requirements of running it."}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Field"}),(0,n.jsx)(t.th,{children:"Notes"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Family name"}),(0,n.jsx)(t.td,{children:"Llama 3"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Type"}),(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"conversational"})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Download site"}),(0,n.jsx)(t.td,{children:(0,n.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/tree/main",children:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/tree/main"})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Version"}),(0,n.jsx)(t.td,{children:(0,n.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/commit/6f6073b423013f6a7d4d9f39144961bfbfbc386b",children:"6f6073b423013f6a7d4d9f39144961bfbfbc386b"})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Storage size"}),(0,n.jsx)(t.td,{children:"140GB"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"GPU count"}),(0,n.jsx)(t.td,{children:"4"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Total GPU memory"}),(0,n.jsx)(t.td,{children:"320GB (~16k context length, more GPUs required for longer context lengths)"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Per GPU memory"}),(0,n.jsx)(t.td,{children:"80GB"})]})]})]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"Requirements",src:s(5196).A+"",width:"1131",height:"1188"})}),"\n",(0,n.jsx)(t.h2,{id:"runtimes",children:"Runtimes"}),"\n",(0,n.jsx)(t.p,{children:"This section describes how to configure the runtime framework to support the inference calls."}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Options"}),(0,n.jsx)(t.th,{children:"Notes"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Runtime"}),(0,n.jsx)(t.td,{children:"vLLM"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Distributed Inference"}),(0,n.jsx)(t.td,{children:"True"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Custom configurations"}),(0,n.jsxs)(t.td,{children:[(0,n.jsx)(t.code,{children:"--max_model_len=16384"})," by default but users with more GPUs can override it in the Workspace configuration if they want to use a longer context length."]})]})]})]}),"\n",(0,n.jsx)(t.h1,{id:"history",children:"History"}),"\n",(0,n.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(t.li,{className:"task-list-item",children:[(0,n.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ","2025-05-29: Open proposal PR."]}),"\n",(0,n.jsxs)(t.li,{className:"task-list-item",children:[(0,n.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ","2025-06-05: Start model integration."]}),"\n",(0,n.jsxs)(t.li,{className:"task-list-item",children:[(0,n.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ","2025-06-05: Complete model support."]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,t,s)=>{s.d(t,{R:()=>a,x:()=>l});var n=s(6540);const r={},i=n.createContext(r);function a(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);