"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[2724],{6128:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var o=t(4848),a=t(8453);const i={title:"Tool Calling"},l=void 0,r={id:"tool-calling",title:"Tool Calling",description:"KAITO supports tool calling, allowing you to integrate external tools into your inference service. This feature enables the model to call APIs or execute functions based on the input it receives, enhancing its capabilities beyond text generation.",source:"@site/versioned_docs/version-v0.5.0/tool-calling.md",sourceDirName:".",slug:"/tool-calling",permalink:"/kaito/docs/v0.5.0/tool-calling",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/versioned_docs/version-v0.5.0/tool-calling.md",tags:[],version:"v0.5.0",frontMatter:{title:"Tool Calling"},sidebar:"sidebar",previous:{title:"Custom Model Integration",permalink:"/kaito/docs/v0.5.0/custom-model"},next:{title:"AWS Deployment",permalink:"/kaito/docs/v0.5.0/aws"}},s={},c=[{value:"Requirements",id:"requirements",level:2},{value:"Supported Inference Runtimes",id:"supported-inference-runtimes",level:3},{value:"Supported Models &amp; Chat Templates",id:"supported-models--chat-templates",level:3},{value:"Inference Configurations",id:"inference-configurations",level:3},{value:"Examples",id:"examples",level:2},{value:"Named Function Calling",id:"named-function-calling",level:3},{value:"Model Context Protocol (MCP)",id:"model-context-protocol-mcp",level:3}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"KAITO supports tool calling, allowing you to integrate external tools into your inference service. This feature enables the model to call APIs or execute functions based on the input it receives, enhancing its capabilities beyond text generation."}),"\n",(0,o.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,o.jsx)(n.h3,{id:"supported-inference-runtimes",children:"Supported Inference Runtimes"}),"\n",(0,o.jsx)(n.p,{children:"Currently, tool calling is only supported with the vLLM inference runtime."}),"\n",(0,o.jsx)(n.h3,{id:"supported-models--chat-templates",children:"Supported Models & Chat Templates"}),"\n",(0,o.jsx)(n.p,{children:"Proper chat templates are required for tool calling. The following models and their corresponding chat templates are supported:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model Family"}),(0,o.jsx)(n.th,{children:"Chat Templates"}),(0,o.jsx)(n.th,{children:"Tool Parser in vLLM"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Phi 4"}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/blob/main/presets/workspace/inference/chat_templates/tool-chat-phi4-mini.jinja",children:"tool-chat-phi4-mini.jinja"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"phi4_mini_json"})})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Llama 3"}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/blob/main/presets/workspace/inference/chat_templates/tool-chat-llama3.1-json.jinja",children:"tool-chat-llama3.1-json.jinja"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"llama3_json"})})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Mistral"}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/blob/main/presets/workspace/inference/chat_templates/tool-chat-mistral.jinja",children:"tool-chat-mistral.jinja"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"mistral"})})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"inference-configurations",children:"Inference Configurations"}),"\n",(0,o.jsxs)(n.p,{children:["Create the following ConfigMap before deploying KAITO workspace, with ",(0,o.jsx)(n.code,{children:"tool-call-parser"})," and ",(0,o.jsx)(n.code,{children:"chat-template"})," set to the appropriate values for your model:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tool-calling-inference-config\ndata:\n  inference_config.yaml: |\n    # Maximum number of steps to find the max available seq len fitting in the GPU memory.\n    max_probe_steps: 6\n\n    vllm:\n      cpu-offload-gb: 0\n      swap-space: 4\n      tool-call-parser: "phi4_mini_json" | "llama3_json" | "mistral"\n      chat-template: "/workspace/chat_templates/<chat_template_name>.jinja"\n'})}),"\n",(0,o.jsxs)(n.p,{children:["In the Workspace configuration, set ",(0,o.jsx)(n.code,{children:".inference.config"})," to the name of the ConfigMap you created. For example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'apiVersion: kaito.sh/v1beta1\nkind: Workspace\nmetadata:\n  name: workspace-phi-4-mini-tool-call\nresource:\n  instanceType: "Standard_NC24ads_A100_v4"\n  labelSelector:\n    matchLabels:\n      apps: phi-4\ninference:\n  preset:\n    name: phi-4-mini-instruct\n  config: tool-calling-inference-config\n'})}),"\n",(0,o.jsxs)(n.p,{children:["For more details on the inference configuration, refer to ",(0,o.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/features/tool_calling.html",children:"vLLM tool calling documentation"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(n.p,{children:"Port-forward the inference service to your local machine:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/workspace-phi-4-mini-tool-call 8000\n"})}),"\n",(0,o.jsx)(n.h3,{id:"named-function-calling",children:"Named Function Calling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")\n\ndef get_weather(location: str, unit: str):\n    return f"Getting the weather for {location} in {unit}..."\ntool_functions = {"get_weather": get_weather}\n\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "get_weather",\n        "description": "Get the current weather in a given location",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "location": {"type": "string", "description": "City and state, e.g., \'San Francisco, CA\'"},\n                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}\n            },\n            "required": ["location", "unit"]\n        }\n    }\n}]\n\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{"role": "user", "content": "What\'s the weather like in San Francisco?"}],\n    tools=tools,\n    tool_choice="auto"\n)\n\ntool_call = response.choices[0].message.tool_calls[0].function\nprint(f"Function called: {tool_call.name}")\nprint(f"Arguments: {tool_call.arguments}")\nprint(f"Result: {tool_functions[tool_call.name](**json.loads(tool_call.arguments))}")\n'})}),"\n",(0,o.jsx)(n.p,{children:"Expected output:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Function called: get_weather\nArguments: {"location": "San Francisco, CA", "unit": "fahrenheit"}\nResult: Getting the weather for San Francisco, CA in fahrenheit...\n'})}),"\n",(0,o.jsx)(n.h3,{id:"model-context-protocol-mcp",children:"Model Context Protocol (MCP)"}),"\n",(0,o.jsxs)(n.p,{children:["With the right client framework, inference workload provisioned by KAITO can also call external tools using the ",(0,o.jsx)(n.a,{href:"https://modelcontextprotocol.io/",children:"Model Context Protocol (MCP)"}),". This allows the model to integrate and share data with external tools, systems, and data sources."]}),"\n",(0,o.jsxs)(n.p,{children:["In the following example, we will use ",(0,o.jsx)(n.a,{href:"https://docs.astral.sh/uv/",children:"uv"})," to create a Python virtual environment and install the necessary dependencies for ",(0,o.jsx)(n.a,{href:"https://microsoft.github.io/autogen/stable//index.html",children:"AutoGen"})," to call the ",(0,o.jsx)(n.a,{href:"https://deepwiki.com/",children:"DeepWiki"})," MCP service and ask questions about the KAITO project."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'mkdir kaito-mcp\ncd kaito-mcp\n# Create and activate a virtual environment\nuv venv create && source .venv/bin/activate\n# Install dependencies\nuv pip install "autogen-ext[openai]" "autogen-agentchat" "autogen-ext[mcp]"\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Create a Python script ",(0,o.jsx)(n.code,{children:"test.py"})," with the following content:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_core.models import ModelFamily, ModelInfo\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import (StreamableHttpMcpToolAdapter,\n                                   StreamableHttpServerParams)\nfrom openai import OpenAI\n\n\nasync def main() -> None:\n    # Create server params for the remote MCP service\n    server_params = StreamableHttpServerParams(\n        url="https://mcp.deepwiki.com/mcp",\n        timeout=30.0,\n        terminate_on_close=True,\n    )\n\n    # Get the ask_question tool from the server\n    adapter = await StreamableHttpMcpToolAdapter.from_server_params(server_params, "ask_question")\n\n    model = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy").models.list().data[0].id\n    model_info: ModelInfo = {\n        "vision": False,\n        "function_calling": True,\n        "json_output": True,\n        "family": ModelFamily.UNKNOWN,\n        "structured_output": True,\n        "multiple_system_messages": True,\n    }\n\n    # Create an agent that can use the ask_question tool\n    model_client = OpenAIChatCompletionClient(base_url="http://localhost:8000/v1", api_key="dummy", model=model, model_info=model_info)\n    agent = AssistantAgent(\n        name="deepwiki",\n        model_client=model_client,\n        tools=[adapter],\n        system_message="You are a helpful assistant.",\n    )\n\n    await Console(\n        agent.run_stream(task="In the GitHub repository \'kaito-project/kaito\', how many preset models are there?", cancellation_token=CancellationToken())\n    )\n\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,o.jsx)(n.p,{children:"To run the script, execute the following command in your terminal:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"uv run test.py\n"})}),"\n",(0,o.jsx)(n.p,{children:"Expected output:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'---------- TextMessage (user) ----------\nIn the GitHub repository \'kaito-project/kaito\', how many preset models are there?\n---------- ToolCallRequestEvent (deepwiki) ----------\n[FunctionCall(id=\'chatcmpl-tool-4e22b15c32d34430b80078a3acc41f0d\', arguments=\'{"repoName": "kaito-project/kaito", "question": "How many preset models are there?"}\', name=\'ask_question\')]\nUnknown SSE event: ping\n---------- ToolCallExecutionEvent (deepwiki) ----------\n[FunctionExecutionResult(content=\'[{"type": "text", "text": "There are 16 preset models in the Kaito project.  These models are defined in the `supported_models.yaml` file  and registered programmatically within the codebase. ...", "annotations": null, "meta": null}]\', name=\'ask_question\', call_id=\'chatcmpl-tool-4e22b15c32d34430b80078a3acc41f0d\', is_error=False)]\n---------- ToolCallSummaryMessage (deepwiki) ----------\n[{"type": "text", "text": "There are 16 preset models in the Kaito project.  These models are defined in the `supported_models.yaml` file  and registered programmatically within the codebase. ...", "annotations": null, "meta": null}]\n'})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function l(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);