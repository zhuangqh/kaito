"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"sidebar":[{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"link","label":"Introduction","href":"/kaito/docs/","docId":"intro","unlisted":false},{"type":"link","label":"Installation","href":"/kaito/docs/installation","docId":"installation","unlisted":false},{"type":"link","label":"Quick Start","href":"/kaito/docs/quick-start","docId":"quick-start","unlisted":false},{"type":"link","label":"Presets","href":"/kaito/docs/presets","docId":"presets","unlisted":false},{"type":"link","label":"Usage","href":"/kaito/docs/usage","docId":"usage","unlisted":false},{"type":"link","label":"FAQ","href":"/kaito/docs/faq","docId":"faq","unlisted":false}],"collapsible":true},{"type":"category","label":"Features","collapsed":false,"items":[{"type":"link","label":"Inference","href":"/kaito/docs/inference","docId":"inference","unlisted":false},{"type":"link","label":"Fine Tuning","href":"/kaito/docs/tuning","docId":"tuning","unlisted":false},{"type":"link","label":"Retrieval-Augmented Generation (RAG)","href":"/kaito/docs/rag","docId":"rag","unlisted":false},{"type":"link","label":"Custom Model Integration","href":"/kaito/docs/custom-model","docId":"custom-model","unlisted":false}],"collapsible":true},{"type":"category","label":"Cloud Providers","collapsed":false,"items":[{"type":"link","label":"AWS Deployment","href":"/kaito/docs/aws","docId":"aws","unlisted":false}],"collapsible":true},{"type":"category","label":"Operations","collapsed":false,"items":[{"type":"link","label":"Monitoring","href":"/kaito/docs/monitoring","docId":"monitoring","unlisted":false},{"type":"link","label":"OOM Prevention","href":"/kaito/docs/kaito-oom-prevention","docId":"kaito-oom-prevention","unlisted":false},{"type":"link","label":"Bring Your Own GPU Nodes","href":"/kaito/docs/kaito-on-byo-gpu-nodes","docId":"kaito-on-byo-gpu-nodes","unlisted":false}],"collapsible":true},{"type":"category","label":"Contributing","collapsed":false,"items":[{"type":"link","label":"Contributing","href":"/kaito/docs/contributing","docId":"contributing","unlisted":false},{"type":"link","label":"Preset onboarding","href":"/kaito/docs/preset-onboarding","docId":"preset-onboarding","unlisted":false},{"type":"link","label":"Proposals","href":"/kaito/docs/proposals","docId":"proposals","unlisted":false}],"collapsible":true}]},"docs":{"aws":{"id":"aws","title":"AWS Deployment","description":"Before you begin, ensure you have the following tools installed:","sidebar":"sidebar"},"contributing":{"id":"contributing","title":"Contributing","description":"This project welcomes contributions and suggestions.","sidebar":"sidebar"},"custom-model":{"id":"custom-model","title":"Custom Model Integration","description":"Option 1: Use Pre-Built Docker Image Without Model Weights","sidebar":"sidebar"},"faq":{"id":"faq","title":"FAQ","description":"How do I ensure preferred nodes are correctly labeled for use in my workspace?","sidebar":"sidebar"},"inference":{"id":"inference","title":"Inference","description":"This document presents how to use the KAITO workspace Custom Resource Definition (CRD) for model serving and serving with LoRA adapters.","sidebar":"sidebar"},"installation":{"id":"installation","title":"Installation","description":"The following guidance assumes Azure Kubernetes Service(AKS) is used to host the Kubernetes cluster. If you want to use Elastic Kubernetes Service (EKS) instead, please follow the installation guide here.","sidebar":"sidebar"},"intro":{"id":"intro","title":"Introduction","description":"Coming soon: KAITO v0.5.0. Retrieval-augmented generation (RAG) - RagEngine support with LlamaIndex orchestration and Faiss as the default vectorDB, learn about recent updates here!","sidebar":"sidebar"},"kaito-on-byo-gpu-nodes":{"id":"kaito-on-byo-gpu-nodes","title":"Bring Your Own GPU Nodes","description":"This guide walks you through deploying KAITO on a Kubernetes cluster with self-provisioned GPU nodes.","sidebar":"sidebar"},"kaito-oom-prevention":{"id":"kaito-oom-prevention","title":"OOM Prevention","description":"Large Language Model (LLM) inference typically requires significant GPU memory. Unlike traditional virtual memory management in operating systems, CUDA does not support GPU memory swapping. This limitation often leads to CUDA Out-Of-Memory (OOM) errors when GPU memory is insufficient for inference requests. Addressing GPU OOM issues is one of the most challenging aspects of LLM deployment and tuning.","sidebar":"sidebar"},"monitoring":{"id":"monitoring","title":"Monitoring","description":"Inference service exposes Prometheus metrics for monitoring system stats.","sidebar":"sidebar"},"preset-onboarding":{"id":"preset-onboarding","title":"Preset onboarding","description":"This document describes how to add a new supported OSS model in KAITO. The process is designed to allow community users to initiate the request. KAITO maintainers will follow up and deal with managing the model images and guiding the code changes to set up the model preset configurations.","sidebar":"sidebar"},"presets":{"id":"presets","title":"Presets","description":"The current supported model families with preset configurations are listed below.","sidebar":"sidebar"},"proposals":{"id":"proposals","title":"Proposals","description":"This section contains proposals for adding new models to KAITO. Each proposal describes the process of evaluating and integrating new OSS models into the KAITO ecosystem.","sidebar":"sidebar"},"proposals/distributed-inference":{"id":"proposals/distributed-inference","title":"Distributed Inference","description":"Distributed Inference"},"proposals/llama-3.3-70b-instruct":{"id":"proposals/llama-3.3-70b-instruct","title":"Proposal for new model support","description":"Add meta-llama/Llama-3.3-70B-Instruct to KAITO supported model list"},"proposals/mistral":{"id":"proposals/mistral","title":"Proposal for Mistral model support","description":"Add mistral-7b to KAITO supported model list."},"proposals/mistral-instruct":{"id":"proposals/mistral-instruct","title":"Proposal for Mistral model support","description":"Add Mistral-7B-Instruct-v0.2 to KAITO supported model list."},"proposals/model-as-oci-artifacts":{"id":"proposals/model-as-oci-artifacts","title":"Distributing LLM Model Files as OCI Artifacts","description":"Summary"},"proposals/phi-2":{"id":"proposals/phi-2","title":"Proposal for Mistral model support","description":"Add phi-2 to KAITO supported model list."},"proposals/phi3-instruct":{"id":"proposals/phi3-instruct","title":"Proposal for new model support","description":"Add Phi-3 Medium Models to KAITO supported model list"},"proposals/phi4-instruct":{"id":"proposals/phi4-instruct","title":"Proposal for support of Microsoft\'s Phi-4","description":"Add official support for Microsoft\'s Phi-4 models in KAITO."},"proposals/qwen2.5-coder":{"id":"proposals/qwen2.5-coder","title":"Proposal for new model support","description":"Add Qwen 2.5 Coder to KAITO supported model list"},"proposals/workspace-subresource-scale-api":{"id":"proposals/workspace-subresource-scale-api","title":"Support scale subresource api for workspace","description":"Support scale subresource api for workspace in kaito"},"proposals/YYYYMMDD-model-template":{"id":"proposals/YYYYMMDD-model-template","title":"Proposal for new model support","description":"- Keep it simple and descriptive. E.g., Add XXXX (model name) to KAITO supported model list."},"quick-start":{"id":"quick-start","title":"Quick Start","description":"After installing KAITO, you can quickly deploy a phi-3.5-mini-instruct inference service to get started.","sidebar":"sidebar"},"rag":{"id":"rag","title":"Retrieval-Augmented Generation (RAG)","description":"This document presents how to use the KAITO ragengine Custom Resource Definition (CRD) for retrieval-augumented generatoin workflow. By creating a RAGEngine resource, you can quickly stand up a service that indexes documents and queries them in conjunction with an existing LLM inference endpoint\u2014no need to custom-build pipelines. This enables your large language model to answer questions based on your own private content.","sidebar":"sidebar"},"tuning":{"id":"tuning","title":"Fine Tuning","description":"This document presents how to use the KAITO workspace Custom Resource Definition (CRD) for parameter-efficient fine-tuning (PEFT) of models, how a Kubernetes job is designed to automate the tuning workflow, and several best practices for troubleshooting.","sidebar":"sidebar"},"usage":{"id":"usage","title":"Usage","description":"The detailed usage for Kaito supported models can be found in HERE. In case users want to deploy their own containerized models, they can provide the pod template in the inference field of the workspace custom resource (please see API definitions for details). The controller will create a deployment workload using all provisioned GPU nodes. Note that currently the controller does NOT handle automatic model upgrade. It only creates inference workloads based on the preset configurations if the workloads do not exist.","sidebar":"sidebar"}}}')}}]);