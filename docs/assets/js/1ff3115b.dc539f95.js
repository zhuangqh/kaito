"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[5232],{9438:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"v0.5.x","label":"v0.5.x","banner":"unmaintained","badge":true,"noIndex":false,"className":"docs-version-v0.5.x","isLast":false,"docsSidebars":{"sidebar":[{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"link","label":"Introduction","href":"/kaito/docs/v0.5.x/","docId":"intro","unlisted":false},{"type":"link","label":"Installation","href":"/kaito/docs/v0.5.x/installation","docId":"installation","unlisted":false},{"type":"link","label":"Quick Start","href":"/kaito/docs/v0.5.x/quick-start","docId":"quick-start","unlisted":false},{"type":"link","label":"Presets","href":"/kaito/docs/v0.5.x/presets","docId":"presets","unlisted":false},{"type":"link","label":"Usage","href":"/kaito/docs/v0.5.x/usage","docId":"usage","unlisted":false},{"type":"link","label":"FAQ","href":"/kaito/docs/v0.5.x/faq","docId":"faq","unlisted":false}],"collapsible":true},{"type":"category","label":"Features","collapsed":false,"items":[{"type":"link","label":"Inference","href":"/kaito/docs/v0.5.x/inference","docId":"inference","unlisted":false},{"type":"link","label":"Multi-Node Inference","href":"/kaito/docs/v0.5.x/multi-node-inference","docId":"multi-node-inference","unlisted":false},{"type":"link","label":"Fine Tuning","href":"/kaito/docs/v0.5.x/tuning","docId":"tuning","unlisted":false},{"type":"link","label":"Retrieval-Augmented Generation (RAG)","href":"/kaito/docs/v0.5.x/rag","docId":"rag","unlisted":false},{"type":"link","label":"Custom Model Integration","href":"/kaito/docs/v0.5.x/custom-model","docId":"custom-model","unlisted":false},{"type":"link","label":"Tool Calling","href":"/kaito/docs/v0.5.x/tool-calling","docId":"tool-calling","unlisted":false},{"type":"link","label":"Model As OCI Artifacts","href":"/kaito/docs/v0.5.x/model-as-oci-artifacts","docId":"model-as-oci-artifacts","unlisted":false}],"collapsible":true},{"type":"category","label":"Cloud Providers","collapsed":false,"items":[{"type":"link","label":"AWS Deployment","href":"/kaito/docs/v0.5.x/aws","docId":"aws","unlisted":false}],"collapsible":true},{"type":"category","label":"Operations","collapsed":false,"items":[{"type":"link","label":"Monitoring","href":"/kaito/docs/v0.5.x/monitoring","docId":"monitoring","unlisted":false},{"type":"link","label":"OOM Prevention","href":"/kaito/docs/v0.5.x/kaito-oom-prevention","docId":"kaito-oom-prevention","unlisted":false},{"type":"link","label":"Bring Your Own GPU Nodes","href":"/kaito/docs/v0.5.x/kaito-on-byo-gpu-nodes","docId":"kaito-on-byo-gpu-nodes","unlisted":false}],"collapsible":true},{"type":"category","label":"Contributing","collapsed":false,"items":[{"type":"link","label":"Contributing","href":"/kaito/docs/v0.5.x/contributing","docId":"contributing","unlisted":false},{"type":"link","label":"Preset onboarding","href":"/kaito/docs/v0.5.x/preset-onboarding","docId":"preset-onboarding","unlisted":false},{"type":"link","label":"Proposals","href":"/kaito/docs/v0.5.x/proposals","docId":"proposals","unlisted":false}],"collapsible":true}]},"docs":{"aws":{"id":"aws","title":"AWS Deployment","description":"Before you begin, ensure you have the following tools installed:","sidebar":"sidebar"},"contributing":{"id":"contributing","title":"Contributing","description":"This project welcomes contributions and suggestions.","sidebar":"sidebar"},"custom-model":{"id":"custom-model","title":"Custom Model Integration","description":"Option 1: Use Pre-Built Docker Image Without Model Weights","sidebar":"sidebar"},"faq":{"id":"faq","title":"FAQ","description":"How do I ensure preferred nodes are correctly labeled for use in my workspace?","sidebar":"sidebar"},"inference":{"id":"inference","title":"Inference","description":"This document presents how to use the KAITO workspace Custom Resource Definition (CRD) for model serving and serving with LoRA adapters.","sidebar":"sidebar"},"installation":{"id":"installation","title":"Installation","description":"The following guidance assumes Azure Kubernetes Service (AKS) is used to host the Kubernetes cluster. If you want to use Elastic Kubernetes Service (EKS) instead, please follow the installation guide here.","sidebar":"sidebar"},"intro":{"id":"intro","title":"Introduction","description":"Retrieval Augmented Generation (RAG) support is live! - KAITO RagEngine uses LlamaIndex and FAISS, learn about it here!","sidebar":"sidebar"},"kaito-on-byo-gpu-nodes":{"id":"kaito-on-byo-gpu-nodes","title":"Bring Your Own GPU Nodes","description":"This guide walks you through deploying KAITO on a Kubernetes cluster with self-provisioned GPU nodes.","sidebar":"sidebar"},"kaito-oom-prevention":{"id":"kaito-oom-prevention","title":"OOM Prevention","description":"Large Language Model (LLM) inference typically requires significant GPU memory. Unlike traditional virtual memory management in operating systems, CUDA does not support GPU memory swapping. This limitation often leads to CUDA Out-Of-Memory (OOM) errors when GPU memory is insufficient for inference requests. Addressing GPU OOM issues is one of the most challenging aspects of LLM deployment and tuning.","sidebar":"sidebar"},"model-as-oci-artifacts":{"id":"model-as-oci-artifacts","title":"Model As OCI Artifacts","description":"Efficiently distribute Large Language Models using Open Container Initiative (OCI) Artifacts","sidebar":"sidebar"},"monitoring":{"id":"monitoring","title":"Monitoring","description":"Inference service exposes Prometheus metrics for monitoring system stats.","sidebar":"sidebar"},"multi-node-inference":{"id":"multi-node-inference","title":"Multi-Node Inference","description":"This document explains how to configure and use multi-node distributed inference in KAITO for large models that require more GPU resources than a single node can provide.","sidebar":"sidebar"},"preset-onboarding":{"id":"preset-onboarding","title":"Preset onboarding","description":"This document describes how to add a new supported OSS model in KAITO. The process is designed to allow community users to initiate the request. KAITO maintainers will follow up and deal with managing the model images and guiding the code changes to set up the model preset configurations.","sidebar":"sidebar"},"presets":{"id":"presets","title":"Presets","description":"The current supported model families with preset configurations are listed below.","sidebar":"sidebar"},"proposals":{"id":"proposals","title":"Proposals","description":"This section contains proposals for adding new models to KAITO. Each proposal describes the process of evaluating and integrating new OSS models into the KAITO ecosystem.","sidebar":"sidebar"},"quick-start":{"id":"quick-start","title":"Quick Start","description":"After installing KAITO, you can quickly deploy a phi-3.5-mini-instruct inference service to get started.","sidebar":"sidebar"},"rag":{"id":"rag","title":"Retrieval-Augmented Generation (RAG)","description":"This document presents how to use the KAITO ragengine Custom Resource Definition (CRD) for retrieval-augumented generatoin workflow. By creating a RAGEngine resource, you can quickly stand up a service that indexes documents and queries them in conjunction with an existing LLM inference endpoint\u2014no need to custom-build pipelines. This enables your large language model to answer questions based on your own private content.","sidebar":"sidebar"},"tool-calling":{"id":"tool-calling","title":"Tool Calling","description":"KAITO supports tool calling, allowing you to integrate external tools into your inference service. This feature enables the model to call APIs or execute functions based on the input it receives, enhancing its capabilities beyond text generation.","sidebar":"sidebar"},"tuning":{"id":"tuning","title":"Fine Tuning","description":"This document presents how to use the KAITO workspace Custom Resource Definition (CRD) for parameter-efficient fine-tuning (PEFT) of models, how a Kubernetes job is designed to automate the tuning workflow, and several best practices for troubleshooting.","sidebar":"sidebar"},"usage":{"id":"usage","title":"Usage","description":"The detailed usage for Kaito supported models can be found in HERE. In case users want to deploy their own containerized models, they can provide the pod template in the inference field of the workspace custom resource (please see API definitions for details). The controller will create a deployment workload using all provisioned GPU nodes. Note that currently the controller does NOT handle automatic model upgrade. It only creates inference workloads based on the preset configurations if the workloads do not exist.","sidebar":"sidebar"}}}}')}}]);