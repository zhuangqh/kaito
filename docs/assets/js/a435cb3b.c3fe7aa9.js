"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[4776],{1926:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/keda-kaito-scaler-bbd436957ac3a1caf6dd2241edae2f75.png"},6490:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var t=r(4848),s=r(8453);const i={title:"Keda Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":new Date("2025-07-02T00:00:00.000Z"),"last-updated":new Date("2025-07-04T00:00:00.000Z"),status:"provisional","see-also":null},a="Title",o={id:"proposals/keda-scaler-for-inference-workloads",title:"Keda Scaler For Inference Workloads In Kaito",description:"Keda Scaler for inference workloads in Kaito",source:"@site/docs/proposals/20250704-keda-scaler-for-inference-workloads.md",sourceDirName:"proposals",slug:"/proposals/keda-scaler-for-inference-workloads",permalink:"/kaito/docs/next/proposals/keda-scaler-for-inference-workloads",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250704-keda-scaler-for-inference-workloads.md",tags:[],version:"current",sidebarPosition:20250704,frontMatter:{title:"Keda Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":"2025-07-02T00:00:00.000Z","last-updated":"2025-07-04T00:00:00.000Z",status:"provisional","see-also":null}},c={},l=[{value:"Summary",id:"summary",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Goals",id:"goals",level:3},{value:"Non-Goals/Future Work",id:"non-goalsfuture-work",level:3},{value:"Proposal",id:"proposal",level:2},{value:"Time-based Scaler of Keda",id:"time-based-scaler-of-keda",level:3},{value:"Example: Business Hours Scaling",id:"example-business-hours-scaling",level:4},{value:"Cron Expression Format",id:"cron-expression-format",level:4},{value:"Metric-based Scaler of Keda",id:"metric-based-scaler-of-keda",level:3},{value:"Prometheus configuration",id:"prometheus-configuration",level:4},{value:"ScaledObject of Keda",id:"scaledobject-of-keda",level:4},{value:"TLS Authentication for Prometheus",id:"tls-authentication-for-prometheus",level:4},{value:"Challenges of this solution",id:"challenges-of-this-solution",level:4},{value:"Keda-Kaito-Scaler Proposal",id:"keda-kaito-scaler-proposal",level:3},{value:"Trigger Specification of Kaito Scaler",id:"trigger-specification-of-kaito-scaler",level:4},{value:"The Implementation of keda-kaito-scaler",id:"the-implementation-of-keda-kaito-scaler",level:4},{value:"External Scaler Interface Design",id:"external-scaler-interface-design",level:5},{value:"Server TLS Configuration For Kaito Scaler",id:"server-tls-configuration-for-kaito-scaler",level:5},{value:"Scaler Manager",id:"scaler-manager",level:5},{value:"Alternatives",id:"alternatives",level:2},{value:"Comparison Overview",id:"comparison-overview",level:3},{value:"Implementation History",id:"implementation-history",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"title",children:"Title"}),"\n",(0,t.jsx)(n.p,{children:"Keda Scaler for inference workloads in Kaito"}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"As the number of waiting inference requests increases, it is necessary to scale more inference instances in order to prevent blocking inference requests. On the other hand, if the number of waiting inference requests declines, we should consider reducing inference instances to improve GPU resource utilization."}),"\n",(0,t.jsx)(n.p,{children:"In this proposal, we hope to enhance the keda system and develop a customized scaler which is specialized for scaling GPU workloads for Kaito. This customized scaler is designed for a minimalistic configuration experience which allows users to easily get started without requiring specialized knowledge of LLM."}),"\n",(0,t.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,t.jsxs)(n.p,{children:["LLM inference service is a basic and widely-used feature in Kaito, and Kaito community interest in auto-scaler for inference workloads continues to intensify. Related issues: ",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/issues/306",children:"#306"}),", ",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/issues/1104",children:"#1104"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"From the technical perspective, we don't want to make a new wheel for auto-scaler, so it's a good idea to expand a customized scaler for keda to dynamically adjusts the number of inference instances based on request volume\u2014scaling up during traffic spikes to improve inference speed, and scaling down during low demand to minimize GPU resource waste. Furthermore, for workloads with predictable, recurring traffic patterns, cron scaler in keda can proactively adjust capacity, ensuring resources are ready before they are needed."}),"\n",(0,t.jsx)(n.p,{children:"The auto-scaler solution for Kaito should be shown as follows:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"keda-kaito-scaler",src:r(1926).A+"",width:"3542",height:"2186"})}),"\n",(0,t.jsx)(n.p,{children:"We will divide this auto-scaler feature into two parts as follows:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Part one: support scale subresource API for workspace, so different auto-scaler solutions such as KEDA, HPA, etc. can be integrated with Kaito to manage inference workloads dynamically. This part is addressed in another proposal: ",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/pull/1184",children:"https://github.com/kaito-project/kaito/pull/1184"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Part two: support a customized scaler(named kaito-scaler) of keda. The kaito scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM. This part will be addressed in this proposal. This scaler supports reactive (metric-based) scaling."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"To ensure ease of use, the specialized kaito scaler is hosted in an independent repo (kaito-project/keda-kaito-scaler). At the same time, the keda-kaito-scaler component can work with Kaito and keda without depending on any other third-party components."}),"\n",(0,t.jsx)(n.h3,{id:"goals",children:"Goals"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"keda-kaito-scaler is a specialized scaler plugin for kada which is specialized for scaling gpu workloads automatically, and can integrate with kaito to work."}),"\n",(0,t.jsx)(n.li,{children:"compared to native prometheus scaler in keda, keda-kaito-scaler can provides the same capability for scaling gpu workloads and with user-friendly and minimalistic configurations."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"non-goalsfuture-work",children:"Non-Goals/Future Work"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The time efficiency of the auto-scaler is not within the scope of this proposal, as it is influenced by mutliple external factors, including GPU node provisioning, LLM image pulling, etc."}),"\n",(0,t.jsx)(n.li,{children:"Only support scale vllm workload, and non-vllm is not covered."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"proposal",children:"Proposal"}),"\n",(0,t.jsx)(n.p,{children:"Keda can provides both metric-based and time-based scaler capability, but for metric-based scaler it should work together with prometheus. The detailed auto-scaler architecture is shown in the following figure:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"keda-prometheus-cron-auto-scaler",src:r(6745).A+"",width:"3666",height:"1805"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Keda"}),": includes two components: metrics-adapter and keda-core. metrics-adpter is used for exposing external metrics on kube-apiserver wihch will be used by native HPA. keda-core includes the core logic of keda system, like the generation of HPA resource according to ScaledObject, and supporting different scalers(like prometheus, cron scalers)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalers"}),": Keda providers more than 100+ scalers, but these scalers are only used for providing metrics for native HPA, and the scaling logic and actions are taken by native HPA."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"HPA"}),": native scaler capability in K8s, scale workloads accroding to HPA resource that created by keda, pull metrics and calculate the desired replicas at a interval time and invokes the ",(0,t.jsx)(n.code,{children:"/scale"})," subresource API of the target workspace when scaling needed."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Prometheus"}),": scrape specified metrics at a interval cycle accroding to PodMonitor configurations."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"time-based-scaler-of-keda",children:"Time-based Scaler of Keda"}),"\n",(0,t.jsxs)(n.p,{children:["reference link: ",(0,t.jsx)(n.a,{href:"https://keda.sh/docs/2.17/scalers/cron/",children:"https://keda.sh/docs/2.17/scalers/cron/"})]}),"\n",(0,t.jsx)(n.p,{children:"The KEDA cron scaler allows you to scale workloads based on time schedules, which is particularly useful for workloads with predictable traffic patterns. This is ideal for scenarios where you know peak hours in advance and want to proactively scale resources before demand increases."}),"\n",(0,t.jsx)(n.h4,{id:"example-business-hours-scaling",children:"Example: Business Hours Scaling"}),"\n",(0,t.jsxs)(n.p,{children:["Below is an example of a ",(0,t.jsx)(n.code,{children:"ScaledObject"})," that scales a Kaito Workspace based on business hours:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scale up to 10 replicas"})," from 6AM to 8PM (peak hours)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scale down to 1 replica"})," otherwise (off-peak hours)"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kaito-workspace-business-hours-scaler\n  namespace: kaito-workloads\nspec:\n  # Target Kaito Workspace to scale\n  scaleTargetRef:\n    apiVersion: kaito.sh/v1alpha1\n    kind: Workspace\n    name: my-vllm-workspace\n\n  # Scaling boundaries\n  minReplicas: 1\n  maxReplicas: 10\n\n  # Cron-based triggers for time-based scaling\n  triggers:\n  # Scale up to 10 replicas at 6AM (start of business hours)\n  - type: cron\n    metadata:\n      timezone: "America/New_York"  # Adjust timezone as needed\n      start: "0 6 * * 1-5"         # 6AM Monday to Friday\n      end: "0 20 * * 1-5"          # 8PM Monday to Friday\n      desiredReplicas: "10"        # Scale to 10 replicas during business hours\n\n  # Scale down to 1 replica at 8PM (end of business hours)\n  - type: cron\n    metadata:\n      timezone: "America/New_York"  # Adjust timezone as needed\n      start: "0 20 * * 1-5"        # 8PM Monday to Friday\n      end: "0 6 * * 1-5"           # 6AM Monday to Friday (next day)\n      desiredReplicas: "1"         # Scale to 1 replica during off-hours\n'})}),"\n",(0,t.jsx)(n.h4,{id:"cron-expression-format",children:"Cron Expression Format"}),"\n",(0,t.jsx)(n.p,{children:"The cron expressions follow the standard format:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"* * * * *\n\u2502 \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 \u2514\u2500 day of week (0-6, Sunday=0)\n\u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 month (1-12)\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500 day of month (1-31)\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0-23)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0-59)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"metric-based-scaler-of-keda",children:"Metric-based Scaler of Keda"}),"\n",(0,t.jsxs)(n.p,{children:["reference link: ",(0,t.jsx)(n.a,{href:"https://keda.sh/docs/2.17/scalers/prometheus/",children:"https://keda.sh/docs/2.17/scalers/prometheus/"})]}),"\n",(0,t.jsx)(n.h4,{id:"prometheus-configuration",children:"Prometheus configuration"}),"\n",(0,t.jsxs)(n.p,{children:["When using the ",(0,t.jsx)(n.a,{href:"https://prometheus-operator.dev/",children:"Prometheus Operator"}),", the recommended way to configure scraping is to create a ",(0,t.jsx)(n.code,{children:"PodMonitor"})," custom resource. This resource declaratively defines how a set of pods should be monitored, and the Prometheus Operator will automatically generate and update the required Prometheus scrape configurations."]}),"\n",(0,t.jsxs)(n.p,{children:["Below is an example of a ",(0,t.jsx)(n.code,{children:"PodMonitor"})," resource designed to discover and scrape metrics from all vLLM inference pods every 15 seconds."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: kaito-vllm-inference-monitor\n  # This PodMonitor should be created in the same namespace as your Prometheus instance,\n  # or in a namespace that Prometheus is configured to watch.\n  namespace: monitoring\n  labels:\n    # This label allows the Prometheus custom resource to discover this PodMonitor.\n    # The key/value may vary depending on your Prometheus Operator setup.\n    release: prometheus\nspec:\n  # Use namespaceSelector to specify which namespaces to look for pods in.\n  # If your vLLM pods are in a specific namespace, list it here.\n  namespaceSelector:\n    matchNames:\n      - kaito-workloads # <-- Change this to the namespace of your inference pods.\n\n  # Use a label selector to identify the target pods to scrape.\n  selector:\n    matchLabels:\n      # This label must be present on your vLLM pods.\n      app.kubernetes.io/component: inference\n      app.kubernetes.io/part-of: kaito\n\n  # podMetricsEndpoints defines the port and path to scrape.\n  podMetricsEndpoints:\n  - port: metrics\n    # This must match the name of the port in the pod's container spec (e.g., `ports.name`).\n    # For vLLM, the container port (e.g., 5000 or 8000) should be named 'metrics'.\n\n    # Override the default scrape interval.\n    interval: 15s\n\n    # The path to the metrics endpoint.\n    path: /metrics\n"})}),"\n",(0,t.jsxs)(n.p,{children:["To make this ",(0,t.jsx)(n.code,{children:"PodMonitor"})," work, we must ensure your vLLM pods are created with the correct metadata:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Labels"}),": The pods must have labels that match the ",(0,t.jsx)(n.code,{children:"selector.matchLabels"})," in the ",(0,t.jsx)(n.code,{children:"PodMonitor"}),". For example:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"metadata:\n  labels:\n    app.kubernetes.io/component: inference\n    app.kubernetes.io/part-of: kaito\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Named Container Port"}),": The container that exposes the metrics endpoint must have a named port that matches the ",(0,t.jsx)(n.code,{children:"port"})," field in the ",(0,t.jsx)(n.code,{children:"podMetricsEndpoints"})," section."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"spec:\n  containers:\n  - name: vllm-inference\n    ports:\n    - containerPort: 5000\n      name: metrics # <-- This name must match the PodMonitor's endpoint port.\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["By using a ",(0,t.jsx)(n.code,{children:"PodMonitor"}),", you can manage the entire lifecycle of your Prometheus scrape configuration through the Kubernetes API, which is more scalable and robust than manually editing configuration files."]}),"\n",(0,t.jsx)(n.h4,{id:"scaledobject-of-keda",children:"ScaledObject of Keda"}),"\n",(0,t.jsxs)(n.p,{children:["Below is a complete example of a KEDA ",(0,t.jsx)(n.code,{children:"ScaledObject"})," that demonstrates how to autoscale a Kaito ",(0,t.jsx)(n.code,{children:"Workspace"})," based on the ",(0,t.jsx)(n.code,{children:"vllm:num_requests_waiting"})," metric from Prometheus. This configuration is specifically optimized for GPU workloads and includes the following key features:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Prometheus Integration"}),": Connects to Prometheus with TLS authentication to query vLLM waiting request metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conservative Scaling"}),": Implements slow, deliberate scaling policies (1 pod every 5 minutes for scale-up, 1 pod every 10 minutes for scale-down) to account for slow GPU node provisioning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dead Zone Configuration"}),": Uses tolerance settings to create distinct high (11) and low (5) thresholds, preventing flapping when metrics fluctuate between these values"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Anti-Flapping Protection"}),": Combines stabilization windows with conservative scaling policies to ensure service stability"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"No Scale-to-Zero"}),": Maintains a minimum of 1 replica to ensure continuous service availability"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kaito-vllm-workspace-scaler\n  # This ScaledObject should be in the same namespace as the Workspace it scales.\n  namespace: kaito-workloads\nspec:\n  # scaleTargetRef points to the Kaito Workspace resource to be scaled.\n  scaleTargetRef:\n    apiVersion: kaito.sh/v1alpha1 # Or v1beta1, depending on your CRD version\n    kind: Workspace\n    name: my-vllm-workspace # <-- Change this to the name of your Workspace\n\n  # Minimum and maximum number of replicas.\n  # minReplicas: 1 prevents KEDA from scaling down to zero.\n  minReplicas: 1\n  maxReplicas: 10\n\n  # Trigger configuration for Prometheus.\n  triggers:\n    - type: prometheus\n      metadata:\n        # The address of the Prometheus server. Use https for TLS.\n        serverAddress: https://prometheus.monitoring.svc:9090\n        # The PromQL query to get the average number of waiting requests per pod.\n        # This query calculates the average metric across all pods belonging to the component.\n        # Ensure your pods have the \'app.kubernetes.io/component: inference\' label.\n        query: |\n          avg(vllm:num_requests_waiting{app.kubernetes.io/component="inference"})\n        # The target value for the metric. HPA will aim to keep the metric at this value.\n        # We set this to 10 to create asymmetric scaling thresholds.\n        threshold: "10"\n        # Specifies that TLS authentication is required to connect to Prometheus.\n        authModes: "tls"\n      # References a TriggerAuthentication resource containing the TLS secrets.\n      authenticationRef:\n        name: keda-prometheus-tls\n\n  # Advanced HPA configuration for fine-grained scaling behavior.\n  advanced:\n    horizontalPodAutoscalerConfig:\n      behavior:\n        # Scale-up behavior: configured to be conservative due to slow GPU provisioning.\n        scaleUp:\n          # A short stabilization window allows for quick reactions to load increases.\n          stabilizationWindowSeconds: 30\n          # Tolerance for scale-up: triggers when metric > threshold * (1 + tolerance)\n          # With threshold: 10 and tolerance: 0.1, scale-up triggers at 11\n          tolerance: 0.1\n          policies:\n            # Only allow scaling up by 1 pod every 5 minutes to ensure stability\n            # during slow GPU node provisioning.\n            - type: Pods\n              value: 1\n              periodSeconds: 300 # 5 minutes\n          # \'Max\' selects the policy that allows the fastest scaling.\n          selectPolicy: Max\n\n        # Scale-down behavior: configured to be very conservative to avoid flapping.\n        scaleDown:\n          # A long stabilization window ensures the load is consistently low before scaling down.\n          stabilizationWindowSeconds: 300\n          # Tolerance for scale-down: triggers when metric < threshold * (1 - tolerance)\n          # With threshold: 10 and tolerance: 0.5, scale-down triggers at 5\n          tolerance: 0.5\n          policies:\n            # Only allow scaling down by 1 pod every 10 minutes to maintain\n            # service availability and prevent aggressive downscaling.\n            - type: Pods\n              value: 1\n              periodSeconds: 600 # 10 minutes\n          selectPolicy: Max\n'})}),"\n",(0,t.jsxs)(n.p,{children:["By setting the ",(0,t.jsx)(n.code,{children:"threshold"})," to ",(0,t.jsx)(n.code,{children:"10"})," and using asymmetric tolerance values, we create more responsive scaling boundaries:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scale-Up Threshold"}),": Scaling up will be triggered when the metric value exceeds ",(0,t.jsx)(n.code,{children:"11"})," (",(0,t.jsx)(n.code,{children:"10 * (1 + 0.1)"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scale-Down Threshold"}),": Scaling down will be triggered when the metric value drops below ",(0,t.jsx)(n.code,{children:"5"})," (",(0,t.jsx)(n.code,{children:"10 * (1 - 0.5)"}),")."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This configuration creates a dead zone between 5 and 11, with more aggressive scale-up behavior (shorter threshold gap) and more conservative scale-down behavior (wider threshold gap). This approach prevents flapping while ensuring quick response to load increases and stability during load decreases."}),"\n",(0,t.jsx)(n.h4,{id:"tls-authentication-for-prometheus",children:"TLS Authentication for Prometheus"}),"\n",(0,t.jsxs)(n.p,{children:["To use TLS authentication, you must create a ",(0,t.jsx)(n.code,{children:"TriggerAuthentication"})," resource and a corresponding Kubernetes ",(0,t.jsx)(n.code,{children:"Secret"})," that holds the client certificate, private key, and CA certificate."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create the Secret"}),":\nFirst, create a secret containing your TLS assets. The keys (",(0,t.jsx)(n.code,{children:"tls.crt"}),", ",(0,t.jsx)(n.code,{children:"tls.key"}),", ",(0,t.jsx)(n.code,{children:"ca.crt"}),") must match the parameters in the ",(0,t.jsx)(n.code,{children:"TriggerAuthentication"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic prometheus-client-secret \\\n  --from-file=tls.crt=/path/to/client.crt \\\n  --from-file=tls.key=/path/to/client.key \\\n  --from-file=ca.crt=/path/to/ca.crt \\\n  -n keda # <-- Create the secret in the KEDA namespace\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create the TriggerAuthentication"}),":\nThis resource tells KEDA how to use the secret to authenticate with Prometheus."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"apiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: keda-prometheus-tls\n  # This resource must be in the same namespace as KEDA itself.\n  namespace: keda\nspec:\n  secretTargetRef:\n    - parameter: tls\n      name: prometheus-client-secret\n      key: tls.crt\n    - parameter: key\n      name: prometheus-client-secret\n      key: tls.key\n    - parameter: ca\n      name: prometheus-client-secret\n      key: ca.crt\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By applying these resources, you establish a secure, robust, and efficient autoscaling system for your Kaito inference workloads."}),"\n",(0,t.jsx)(n.h4,{id:"challenges-of-this-solution",children:"Challenges of this solution"}),"\n",(0,t.jsx)(n.p,{children:"While the KEDA + Prometheus approach provides powerful autoscaling capabilities for vllm workloads, it presents several significant challenges for end users:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Complex Setup and Configuration"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-component architecture"}),": Users must deploy and configure Prometheus Operator, KEDA, and multiple custom resources (",(0,t.jsx)(n.code,{children:"PodMonitor"}),", ",(0,t.jsx)(n.code,{children:"ScaledObject"}),", ",(0,t.jsx)(n.code,{children:"TriggerAuthentication"}),", ",(0,t.jsx)(n.code,{children:"Secret"}),")"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configuration complexity"}),": Each component requires detailed configuration with specific labels, selectors, and parameters that must align correctly"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Monitoring Stack Dependency"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Prometheus requirement"}),": Users must deploy and maintain a full Prometheus monitoring stack, which adds operational overhead and resource consumption"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operator knowledge"}),": Requires understanding of Prometheus Operator concepts like ",(0,t.jsx)(n.code,{children:"ServiceMonitor"}),", ",(0,t.jsx)(n.code,{children:"PodMonitor"}),", and ",(0,t.jsx)(n.code,{children:"PrometheusRule"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage and retention"}),": Prometheus requires persistent storage and proper retention policies, adding to infrastructure costs"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"3. Security Configuration Complexity"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TLS certificate management"}),": Setting up TLS authentication requires generating, distributing, and rotating client certificates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Secret management"}),": Multiple secrets must be created and maintained across different namespaces"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"4. Domain Knowledge Requirements"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Prometheus expertise"}),": Users need to understand PromQL, metric types, and Prometheus configuration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kubernetes scaling knowledge"}),": Deep understanding of HPA behavior, scaling policies, and stabilization windows is required"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM-specific tuning"}),": Users must understand vLLM metrics and how they relate to inference performance"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["These challenges highlight the need for a more user-friendly, domain-specific solution that abstracts away the complexity while providing the same autoscaling capabilities. This is the primary motivation for developing the specialized ",(0,t.jsx)(n.code,{children:"keda-kaito-scaler"})," plugin proposed in this document."]}),"\n",(0,t.jsx)(n.h3,{id:"keda-kaito-scaler-proposal",children:"Keda-Kaito-Scaler Proposal"}),"\n",(0,t.jsx)(n.p,{children:"In order to address the above challenges, we consider to create a new component named keda-kaito-scaler to provide features as follows:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["kaito scaler: works as an external scaler of keda, and collects metrics from inference pods according to ScaledObject configurations. This means that ",(0,t.jsx)(n.code,{children:"PodMonitor"}),", ",(0,t.jsx)(n.code,{children:"TriggerAuthentication"}),", ",(0,t.jsx)(n.code,{children:"Secret"})," configurations aren't needed, and users only need to configure ",(0,t.jsx)(n.code,{children:"ScaledObject"}),". at the same time, users don't need to maintain the Prometheus stack."]}),"\n",(0,t.jsxs)(n.li,{children:["kaito scaler manager: works as a deployment, includes webhooks for configuring default values for ",(0,t.jsx)(n.code,{children:"ScaledObject"}),", so users don't need to dive into the details of HPA behavior or vllm metrics. and controllers for ensuring secret which used by grpc connection between keda core and kaito scaler(external scaler)."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"keda-kaito-scaler-arch",src:r(7865).A+"",width:"3730",height:"1782"})}),"\n",(0,t.jsx)(n.h4,{id:"trigger-specification-of-kaito-scaler",children:"Trigger Specification of Kaito Scaler"}),"\n",(0,t.jsx)(n.p,{children:"The Kaito external scaler provides a simplified configuration interface for scaling vLLM inference workloads. Unlike the Prometheus scaler, it directly scrapes metrics from inference pods, eliminating the need for a separate monitoring stack."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'triggers:\n- type: external\n  metadata:\n    # Required fields\n\n    # Unique identifier for this scaler instance\n    scalerName: keda-kaito-scaler\n\n    # threshold for scaling up/down\n    # When average waiting requests per pod exceeds this value * (1 + scaleUp.tolerance), scale up\n    # When average waiting requests per pod drops below this value * (1 - scaleDown.tolerance), scale down\n    threshold: "10"\n\n    # Optional fields:\n\n    # The name of the workspace resource\n    # Default: name of the ScaledObject.Spec.scaleTargetRef\n    workspaceName: my-vllm-workspace\n\n    # The namespace of the workspace resource\n    # Default: namespace of the ScaledObject object.\n    workspaceNamespace: kaito-workloads\n\n    # The address of the external scaler\n    scalerAddress: kaito-scaler.keda.svc.cluster.local:9090\n\n    # Metric name to scrape from pods\n    # Default: "vllm:num_requests_waiting"\n    metricName: "vllm:num_requests_waiting"\n\n    # Protocol for scraping metrics from pods\n    # Default: "https"\n    metricPorotocol: "https"\n\n    # Port name for scraping metrics from pods\n    # Default: "5000"\n    metricPort: "5000"\n\n    # Path for metrics endpoint on pods\n    # Default: "/metrics"\n    metricPath: "/metrics"\n\n    # Optional: Timeout for metric scraping in seconds\n    # Default: 5\n    scrapeTimeout: "5"\n  # Optional: TLS Authentication used by keda-core to acess keda-kaito-scaler\n  # Default: "keda-kaito-creds"\n  authenticationRef:\n    name: keda-kaito-creds\n    kind: ClusterTriggerAuthentication\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example ScaledObject with Kaito External Scaler"})}),"\n",(0,t.jsx)(n.p,{children:"Here's a complete example showing how to use the Kaito external scaler:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kaito-vllm-workspace-scaler\n  namespace: kaito-workloads\nspec:\n  # Target Kaito Workspace to scale\n  scaleTargetRef:\n    apiVersion: kaito.sh/v1alpha1\n    kind: Workspace\n    name: my-vllm-workspace\n\n  # Scaling boundaries\n  minReplicas: 1\n  maxReplicas: 10\n\n  # Simplified trigger configuration - no Prometheus required!\n  triggers:\n  - type: external\n    metadata:\n      scalerName: keda-kaito-scaler\n      metricName: "vllm:num_requests_waiting"\n      threshold: "10"\n      # All other settings use sensible defaults\n\n  # Optional: Advanced HPA behavior (can be omitted for defaults)\n  advanced:\n    horizontalPodAutoscalerConfig:\n      behavior:\n        scaleUp:\n          stabilizationWindowSeconds: 30\n          tolerance: 0.1\n          policies:\n          - type: Pods\n            value: 1\n            periodSeconds: 300 # 5 minutes\n        scaleDown:\n          stabilizationWindowSeconds: 300\n          tolerance: 0.5\n          policies:\n          - type: Pods\n            value: 1\n            periodSeconds: 600 # 10 minutes\n'})}),"\n",(0,t.jsx)(n.h4,{id:"the-implementation-of-keda-kaito-scaler",children:"The Implementation of keda-kaito-scaler"}),"\n",(0,t.jsx)(n.p,{children:"The keda-kaito-scaler consists of two main components: an external scaler that implements the KEDA external scaler interface, and a manager that handles Kubernetes resources and lifecycle management."}),"\n",(0,t.jsx)(n.h5,{id:"external-scaler-interface-design",children:"External Scaler Interface Design"}),"\n",(0,t.jsxs)(n.p,{children:["The detailed interface for external scaler of keda: ",(0,t.jsx)(n.a,{href:"https://keda.sh/docs/2.17/concepts/external-scalers/#external-scaler-grpc-interface",children:"https://keda.sh/docs/2.17/concepts/external-scalers/#external-scaler-grpc-interface"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-go",children:'// External Scaler GRPC Service Interface\n// This implements the KEDA external scaler protocol\ntype ExternalScaler interface {\n    // IsActive determines if the scaler should be active for a given ScaledObject\n    IsActive(ctx context.Context, req *pb.ScaledObjectRef) (*pb.IsActiveResponse, error)\n\n    // GetMetricSpec returns the metric specification for HPA\n    GetMetricSpec(ctx context.Context, req *pb.ScaledObjectRef) (*pb.GetMetricSpecResponse, error)\n\n    // GetMetrics returns the current metric values\n    GetMetrics(ctx context.Context, req *pb.GetMetricsRequest) (*pb.GetMetricsResponse, error)\n}\n\n// Kaito Scaler Implementation\ntype KaitoScaler struct {\n    kubeClient    client.Client\n    httpClient    *http.Client\n}\n\n// IsActive checks if the target Kaito Workspace is ready for scaling\nfunc (k *KaitoScaler) IsActive(ctx context.Context, req *pb.ScaledObjectRef) (*pb.IsActiveResponse, error) {\n    // Get the target Kaito Workspace\n\tvar workspace kaito.Workspace\n    err := k.Get(ctx, req.Namespace/Name, &workspace)\n    if err != nil {\n        return &pb.IsActiveResponse{Result: false}, nil\n    }\n\n    // Check if workspace is ready and has inference enabled\n    isActive := workspace.Status.Phase == kaitov1alpha1.WorkspacePhaseReady &&\n               workspace.Spec.Inference != nil &&\n               workspace.Spec.Inference.Replicas > 0\n\n    return &pb.IsActiveResponse{Result: isActive}, nil\n}\n\n// GetMetricSpec returns the metric specification for this scaler\nfunc (k *KaitoScaler) GetMetricSpec(ctx context.Context, req *pb.ScaledObjectRef) (*pb.GetMetricSpecResponse, error) {\n    return &pb.GetMetricSpecResponse{\n        MetricSpecs: []*pb.MetricSpec{\n            {\n                MetricName:   fmt.Sprintf("s%d-%s", req.TriggerIndex, req.MetricName),\n                TargetSize:   req.Threshold,\n                MetricType:   pb.MetricType_AverageValue,\n            },\n        },\n    }, nil\n}\n\n// GetMetrics retrieves current metrics from inference pods\nfunc (k *KaitoScaler) GetMetrics(ctx context.Context, req *pb.GetMetricsRequest) (*pb.GetMetricsResponse, error) {\n    // Parse scaler metadata from the request\n    scalerConfig, err := k.parseScalerMetadata(req.ScaledObjectRef, req.MetricName)\n    if err != nil {\n        return nil, fmt.Errorf("failed to parse scaler metadata: %w", err)\n    }\n\n    // Get target Kaito Workspace\n\tvar workspace kaito.Workspace\n    err := k.Get(ctx, req.Namespace/Name, &workspace)\n    if err != nil {\n        return &pb.IsActiveResponse{Result: false}, nil\n    }\n\n    // Get inference pods for this workspace\n    pods, err := k.getInferencePods(ctx, workspace)\n    if err != nil {\n        return nil, fmt.Errorf("failed to get inference pods: %w", err)\n    }\n\n    // Collect metrics from all pods with intelligent fallback for missing metrics\n    var totalValue float64\n    var podCount int\n    var readyPodCount int\n    var missingMetricCount int\n\n    for _, pod := range pods {\n        podCount++\n        if k.isPodReady(pod) {\n            readyPodCount++\n            value, err := k.getMetricFromPod(ctx, pod, scalerConfig)\n            if err != nil {\n                // Pod is ready but metric is unavailable (e.g., endpoint not responding)\n                missingMetricCount++\n                continue\n            }\n            totalValue += float64(value)\n        } else {\n            // Pod is not ready (pending, starting, etc.)\n            missingMetricCount++\n        }\n    }\n\n    // Apply fallback for missing metrics to prevent flapping\n    if missingMetricCount > 0 {\n        fallbackValue := k.calculateFallbackValue(scalerConfig, totalValue, readyPodCount-missingMetricCount)\n        totalValue += fallbackValue * float64(missingMetricCount)\n    }\n\n    // Calculate average metric value per pod (including fallback values)\n    var avgValue float64\n    if podCount > 0 {\n        avgValue = totalValue / float64(podCount)\n    }\n\n    return &pb.GetMetricsResponse{\n        MetricValues: []*pb.MetricValue{\n            {\n                MetricName:  req.MetricName,\n                MetricValue: int64(avgValue),\n            },\n        },\n    }, nil\n}\n\n// calculateFallbackValue determines the fallback metric value for missing pods\n// to prevent scaling flapping based on the current scaling direction\nfunc (k *KaitoScaler) calculateFallbackValue(config *ScalerConfig, currentTotal float64, validPodCount int) float64 {\n    // Calculate current average from valid pods\n    var currentAvg float64\n    if validPodCount > 0 {\n        currentAvg = currentTotal / float64(validPodCount)\n    }\n\n    // Determine scaling direction based on current average vs threshold\n    threshold := config.Threshold\n    if currentAvg > threshold {\n        // Scale-up scenario: current load is high\n        // Use 0 for missing pods to avoid overestimating load\n        // This prevents unnecessary aggressive scale-up due to missing metrics\n        return 0\n    } else {\n        // Scale-down scenario or steady state: current load is low/normal\n        // Use high threshold value for missing pods to be conservative\n        // This prevents premature scale-down when some pods are not reporting metrics\n        return threshold * 1.5 // Use 1.5x threshold as "high" value to ensure conservative scaling\n    }\n}\n\ntype ScalerConfig struct {\n    MetricName    string\n    MetricsPort   string\n    MetricsPath   string\n    ScrapeTimeout time.Duration\n    Threshold     float64\n}\n\nfunc (k *KaitoScaler) parseScalerMetadata(ref *pb.ScaledObjectRef, metricName string) (*ScalerConfig, error) {\n    // Parse metadata from ScaledObject (passed via KEDA)\n    // This would extract values like metricsPort, metricsPath, threshold, etc.\n    config := &ScalerConfig{\n        MetricName:    "vllm:num_requests_waiting", // default\n        MetricsPort:   "5000",                      // default vLLM port\n        MetricsPath:   "/metrics",                  // default\n        ScrapeTimeout: 5 * time.Second,             // default\n        Threshold:     10,                          // default threshold for scaling decisions\n    }\n\n    // Override with values from ScaledObject metadata if provided\n    // Implementation would parse ref.ScalerMetadata map to extract:\n    // - threshold: scaling threshold value\n    // - metricPort: port for metrics scraping\n    // - metricPath: path for metrics endpoint\n    // - scrapeTimeout: timeout for metric collection\n\n    return config, nil\n}\n'})}),"\n",(0,t.jsx)(n.h5,{id:"server-tls-configuration-for-kaito-scaler",children:"Server TLS Configuration For Kaito Scaler"}),"\n",(0,t.jsxs)(n.p,{children:["The Kaito external scaler implements a secure gRPC server that uses TLS certificates from the ",(0,t.jsx)(n.code,{children:"keda-kaito-scaler-certs"})," secret for encrypted communication with KEDA core."]}),"\n",(0,t.jsxs)(n.p,{children:["The external scaler gRPC server must be configured to use the server certificates from the ",(0,t.jsx)(n.code,{children:"keda-kaito-scaler-certs"})," secret. Since the secret is created by the same pod, we can't mount it as a volume. Instead, we use a Kubernetes client with a secret lister in the ",(0,t.jsx)(n.code,{children:"GetCertificate"})," callback to dynamically retrieve certificates:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-go",children:'// CertificateManager manages TLS certificates using Kubernetes informers for efficient caching\ntype CertificateManager struct {\n    secretLister    corelisters.SecretLister\n    secretNamespace string\n}\n\n// getSecret retrieves the certificate secret using the informer lister (cached)\nfunc (cm *CertificateManager) getSecret() (*corev1.Secret, error) {\n    secret, err := cm.secretLister.Secrets(cm.secretNamespace).Get(cm."keda-kaito-scaler-certs")\n    if err != nil {\n        return nil, fmt.Errorf("failed to get certificate secret from cache: %w", err)\n    }\n    return secret, nil\n}\n\n// GetServerCertificate retrieves server certificate using cached informer data\nfunc (cm *CertificateManager) GetServerCertificate() (*tls.Certificate, error) {\n    secret, err := cm.getSecret()\n    if err != nil {\n        return nil, err\n    }\n\n    // Extract server certificate and key\n    serverCertPEM, exists := secret.Data["server.crt"]\n    if !exists {\n        return nil, fmt.Errorf("server.crt not found in secret")\n    }\n\n    serverKeyPEM, exists := secret.Data["server.key"]\n    if !exists {\n        return nil, fmt.Errorf("server.key not found in secret")\n    }\n\n    // Parse the certificate and key\n    cert, err := tls.X509KeyPair(serverCertPEM, serverKeyPEM)\n    if err != nil {\n        return nil, fmt.Errorf("failed to parse server certificate: %w", err)\n    }\n\n    return &cert, nil\n}\n\n// GetCAPool retrieves CA certificate pool using cached informer data\nfunc (cm *CertificateManager) GetCAPool() (*x509.CertPool, error) {\n    secret, err := cm.getSecret()\n    if err != nil {\n        return nil, err\n    }\n\n    // Extract CA certificate\n    caCertPEM, exists := secret.Data["ca.crt"]\n    if !exists {\n        return nil, fmt.Errorf("ca.crt not found in secret")\n    }\n\n    // Create certificate pool and add CA\n    caCertPool := x509.NewCertPool()\n    if !caCertPool.AppendCertsFromPEM(caCertPEM) {\n        return nil, fmt.Errorf("failed to parse CA certificate")\n    }\n\n    return caCertPool, nil\n}\n\n// TLS Configuration for gRPC Server with informer-based certificate retrieval\nfunc setupTLSServer(certManager *CertificateManager) (*grpc.Server, error) {\n    // Configure TLS with dynamic certificate loading using informer cache\n    tlsConfig := &tls.Config{\n        // GetCertificate dynamically loads server certificate from informer cache\n        // This allows automatic pickup of renewed certificates without restart\n        GetCertificate: func(*tls.ClientHelloInfo) (*tls.Certificate, error) {\n            return certManager.GetServerCertificate()\n        },\n        GetConfigForClient: func(*tls.ClientHelloInfo) (*tls.Config, error) {\n            // Dynamically load CA certificate for client verification\n            caCertPool, err := certManager.GetCAPool()\n            if err != nil {\n                return nil, fmt.Errorf("failed to load CA certificate: %w", err)\n            }\n\n            return &tls.Config{\n                ClientAuth: tls.RequireAndVerifyClientCert,\n                ClientCAs:  caCertPool,\n                MinVersion: tls.VersionTLS12,\n            }, nil\n        },\n        MinVersion: tls.VersionTLS12,\n    }\n\n    // Create gRPC server with TLS credentials\n    creds := credentials.NewTLS(tlsConfig)\n    server := grpc.NewServer(grpc.Creds(creds))\n    return server, nil\n}\n'})}),"\n",(0,t.jsx)(n.h5,{id:"scaler-manager",children:"Scaler Manager"}),"\n",(0,t.jsxs)(n.p,{children:["The Scaler Manager includes a ",(0,t.jsx)(n.strong,{children:"Scaler Webhook"})," that provides defaults for ScaledObject configurations targeting Kaito Workspaces. and a ",(0,t.jsx)(n.strong,{children:"Scaler Controller"})," for ensuring certificates for tls connection between keda-core and kaito scaler."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Scaler Webhook"})}),"\n",(0,t.jsx)(n.p,{children:"The webhook serves as a mutating admission controller that automatically applies GPU-optimized defaults to ScaledObjects."}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Configuration"}),(0,t.jsx)(n.th,{children:"Value"}),(0,t.jsx)(n.th,{children:"Rationale"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"minReplicaCount: 1"})}),(0,t.jsx)(n.td,{children:"Never scale to zero"}),(0,t.jsx)(n.td,{children:"Ensures continuous inference availability; avoids cold start delays"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"scaleUp.periodSeconds: 300"})}),(0,t.jsx)(n.td,{children:"5 minutes"}),(0,t.jsx)(n.td,{children:"Accounts for slow GPU node provisioning and container startup"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"scaleDown.periodSeconds: 600"})}),(0,t.jsx)(n.td,{children:"10 minutes"}),(0,t.jsx)(n.td,{children:"Very conservative to prevent aggressive downscaling of expensive resources"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"scaleUp.stabilizationWindowSeconds: 30"})}),(0,t.jsx)(n.td,{children:"30 seconds"}),(0,t.jsx)(n.td,{children:"Quick response to load increases for better user experience"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"scaleDown.stabilizationWindowSeconds: 300"})}),(0,t.jsx)(n.td,{children:"5 minutes"}),(0,t.jsx)(n.td,{children:"Ensures load is consistently low before scaling down"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'metricProtocol: "https"'})}),(0,t.jsx)(n.td,{children:"https"}),(0,t.jsx)(n.td,{children:"Standard vLLM metrics protocol"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'metricPort: "5000"'})}),(0,t.jsx)(n.td,{children:"Port 5000"}),(0,t.jsx)(n.td,{children:"Standard vLLM metrics port"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'metricPath: "/metrics"'})}),(0,t.jsx)(n.td,{children:"/metrics"}),(0,t.jsx)(n.td,{children:"Standard vLLM metrics path"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'scrapeTimeout: "5"'})}),(0,t.jsx)(n.td,{children:"5 seconds"}),(0,t.jsx)(n.td,{children:"Reasonable timeout for metrics collection"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'authenticationRef.name: "keda-kaito-creds"'})}),(0,t.jsx)(n.td,{children:"keda-kaito-creds"}),(0,t.jsx)(n.td,{children:"ClusterTriggerAuthentication used for TLS authentication between keda-core and keda-kaito-scaler across namespaces"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'authenticationRef.kind: "ClusterTriggerAuthentication"'})}),(0,t.jsx)(n.td,{children:"ClusterTriggerAuthentication"}),(0,t.jsx)(n.td,{children:"all scaledObjects use the same credentials"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Scaler Controller"})}),"\n",(0,t.jsx)(n.p,{children:"The Scaler Controller is responsible for managing TLS certificates and authentication resources required for secure GRPC communication between KEDA core and the external Kaito scaler. This controller ensures that certificates, secrets, and authentication resources are automatically generated, distributed, and renewed without manual intervention."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Certificate Structure:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CA Certificate"}),": Root certificate authority for the scaler communication"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Server Certificate"}),": Used by the external Kaito scaler GRPC server"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Client Certificate"}),": Used by KEDA core to authenticate with the external scaler"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DNS SANs"}),": Includes all necessary service names and IPs for flexible deployment(like: scaler service FQDN: kaito-scaler.keda.svc.cluster.local)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Managed Resources:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Secret Resource (",(0,t.jsx)(n.code,{children:"keda-kaito-scaler-certs"}),")"]}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: keda-kaito-scaler-certs\n  namespace: keda\ntype: kubernetes.io/tls\ndata:\n  ca.crt: <base64-encoded-ca-certificate>\n  tls.crt: <base64-encoded-client-certificate>\n  tls.key: <base64-encoded-client-private-key>\n  server.crt: <base64-encoded-server-certificate>\n  server.key: <base64-encoded-server-private-key>\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["ClusterTriggerAuthentication Resource (",(0,t.jsx)(n.code,{children:"keda-kaito-creds"}),")"]}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"apiVersion: keda.sh/v1alpha1\nkind: ClusterTriggerAuthentication\nmetadata:\n  name: keda-kaito-creds\nspec:\n  secretTargetRef:\n    - parameter: caCert\n      name: keda-kaito-scaler-certs\n      key: ca.crt\n    - parameter: tlsClientCert\n      name: keda-kaito-scaler-certs\n      key: tls.crt\n    - parameter: tlsClientKey\n      name: keda-kaito-scaler-certs\n      key: tls.key\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Controller Responsibilities:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Certificate Generation"}),": Automatically generates CA, server, and client certificates with appropriate DNS SANs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Secret Management"}),": Creates and maintains the ",(0,t.jsx)(n.code,{children:"keda-kaito-scaler-certs"})," secret in the same namespace as KEDA core (typically ",(0,t.jsx)(n.code,{children:"keda"})," namespace) with all necessary certificate data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ClusterTriggerAuthentication Management"}),": Ensures the ",(0,t.jsx)(n.code,{children:"keda-kaito-creds"})," ClusterTriggerAuthentication resource exists and references the ",(0,t.jsx)(n.code,{children:"keda-kaito-scaler-certs"})," secret"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Certificate Renewal"}),": Monitors certificate expiration and automatically renews certificates before they expire"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Namespace Coordination"}),": Manages resources in the KEDA namespace while providing cluster-wide authentication for all scaledObjects that are targeting Kaito workloads."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"alternatives",children:"Alternatives"}),"\n",(0,t.jsx)(n.p,{children:"This section compares three different auto-scaling solutions for LLM inference workloads in Kaito: KEDA + Prometheus, KEDA + Kaito Scaler, and LLMAutoScaler."}),"\n",(0,t.jsx)(n.h3,{id:"comparison-overview",children:"Comparison Overview"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"KEDA + Prometheus"}),(0,t.jsx)(n.th,{children:"KEDA + Kaito Scaler(Proposed)"}),(0,t.jsx)(n.th,{children:"LLMAutoScaler"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Complexity"})}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Medium"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Setup Time"})}),(0,t.jsx)(n.td,{children:"Hours"}),(0,t.jsx)(n.td,{children:"Minutes"}),(0,t.jsx)(n.td,{children:"Minutes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Dependencies"})}),(0,t.jsx)(n.td,{children:"Prometheus Stack + KEDA"}),(0,t.jsx)(n.td,{children:"KEDA Only"}),(0,t.jsx)(n.td,{children:"Custom Implementation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Configuration Lines"})}),(0,t.jsx)(n.td,{children:"100+ YAML lines"}),(0,t.jsx)(n.td,{children:"10-15 YAML lines"}),(0,t.jsx)(n.td,{children:"10-15 YAML lines"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Domain Knowledge Required"})}),(0,t.jsx)(n.td,{children:"PromQL, HPA, TLS, Monitoring"}),(0,t.jsx)(n.td,{children:"Basic KEDA concepts"}),(0,t.jsx)(n.td,{children:"Custom scaling logic"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Maintenance Overhead"})}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Medium"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Production Readiness"})}),(0,t.jsx)(n.td,{children:"High (with effort)"}),(0,t.jsx)(n.td,{children:"High (out-of-box)"}),(0,t.jsx)(n.td,{children:"Depends on implementation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Vendor Lock-in"})}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"High (Kaito-specific)"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-history",children:"Implementation History"}),"\n",(0,t.jsx)(n.p,{children:"07/07/2025: Open proposal PR"})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},6745:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/keda-prometheus-cron-auto-scaler-515840771e093fd02d54d67f8c3d96c6.png"},7865:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/keda-kaito-scaler-arch-446e6cd5aa8f32aa8d9e69d604f11f12.png"},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);