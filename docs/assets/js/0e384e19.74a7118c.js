"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[3976],{2053:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"intro","title":"Introduction","description":"Retrieval Augmented Generation (RAG) support is live! - KAITO RagEngine uses LlamaIndex and FAISS, learn about it here!","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/kaito/docs/next/","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/docs/intro.md","tags":[],"version":"current","frontMatter":{"title":"Introduction","slug":"/"},"sidebar":"sidebar","next":{"title":"Installation","permalink":"/kaito/docs/next/installation"}}');var i=t(74848),s=t(28453);const o={title:"Introduction",slug:"/"},c=void 0,a={},l=[{value:"Key Features",id:"key-features",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Community",id:"community",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.admonition,{title:"What's NEW!",type:"info",children:[(0,i.jsxs)(n.p,{children:["Retrieval Augmented Generation (RAG) support is live! - KAITO RagEngine uses LlamaIndex and FAISS, learn about it ",(0,i.jsx)(n.a,{href:"https://kaito-project.github.io/kaito/docs/rag",children:"here"}),"!\n",(0,i.jsx)(n.strong,{children:"Latest Release:"})," Sept 23th, 2025. KAITO v0.7.0."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"First Release:"})," Nov 15th, 2023. KAITO v0.1.0."]})]}),"\n",(0,i.jsxs)(n.p,{children:["KAITO is an operator that automates the AI/ML model inference or tuning workload in a Kubernetes cluster.\nThe target models are popular open-sourced large models such as ",(0,i.jsx)(n.a,{href:"https://huggingface.co/tiiuae",children:"falcon"})," and ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main/en/model_doc/phi3",children:"phi-3"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsx)(n.p,{children:"KAITO has the following key differentiations compared to most of the mainstream model deployment methodologies built on top of virtual machine infrastructures:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Container-based Model Management"}),": Manage large model files using container images with an OpenAI-compatible server for inference calls"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preset Configurations"}),": Avoid adjusting workload parameters based on GPU hardware with built-in configurations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multiple Runtime Support"}),": Support for popular inference runtimes including ",(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," and ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/transformers",children:"transformers"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Auto-provisioning"}),": Automatically provision GPU nodes based on model requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Public Registry"}),": Host large model images in the public Microsoft Container Registry (MCR) when licenses allow"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Using KAITO, the workflow of onboarding large AI inference models in Kubernetes is largely simplified."}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsxs)(n.p,{children:["KAITO follows the classic Kubernetes Custom Resource Definition(CRD)/controller design pattern. Users manage a ",(0,i.jsx)(n.code,{children:"workspace"})," custom resource which describes the GPU requirements and the inference or tuning specification. KAITO controllers automate the deployment by reconciling the ",(0,i.jsx)(n.code,{children:"workspace"})," custom resource."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"KAITO architecture",src:t(30679).A+"",width:"2666",height:"1123"})}),"\n",(0,i.jsx)(n.p,{children:"The above figure presents the KAITO architecture overview. Its major components consist of:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Workspace controller"}),": Reconciles the ",(0,i.jsx)(n.code,{children:"workspace"})," custom resource, creates ",(0,i.jsx)(n.code,{children:"machine"})," custom resources to trigger node auto provisioning, and creates the inference or tuning workload (",(0,i.jsx)(n.code,{children:"deployment"}),", ",(0,i.jsx)(n.code,{children:"statefulset"})," or ",(0,i.jsx)(n.code,{children:"job"}),") based on the model preset configurations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Node provisioner controller"}),": The controller's name is ",(0,i.jsx)(n.em,{children:"gpu-provisioner"})," in ",(0,i.jsx)(n.a,{href:"https://github.com/Azure/gpu-provisioner/tree/main/charts/gpu-provisioner",children:"gpu-provisioner helm chart"}),". It uses the ",(0,i.jsx)(n.code,{children:"machine"})," CRD originated from ",(0,i.jsx)(n.a,{href:"https://sigs.k8s.io/karpenter",children:"Karpenter"})," to interact with the workspace controller. It integrates with Azure Resource Manager REST APIs to add new GPU nodes to the AKS or AKS Arc cluster."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://github.com/Azure/gpu-provisioner",children:(0,i.jsx)(n.em,{children:"gpu-provisioner"})})," is an open sourced component. It can be replaced by other controllers if they support ",(0,i.jsx)(n.a,{href:"https://sigs.k8s.io/karpenter",children:"Karpenter-core"})," APIs."]})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NEW!"})," Starting with version v0.5.0, KAITO releases a new operator, ",(0,i.jsx)(n.strong,{children:"RAGEngine"}),", which is used to streamline the process of managing a Retrieval Augmented Generation(RAG) service.\n",(0,i.jsx)(n.img,{alt:"KAITO RAGEngine architecture",src:t(52325).A+"",width:"2617",height:"1124"})]}),"\n",(0,i.jsxs)(n.p,{children:["As illustrated in the above figure, the ",(0,i.jsx)(n.strong,{children:"RAGEngine controller"})," reconciles the ",(0,i.jsx)(n.code,{children:"ragengine"})," custom resource and creates a ",(0,i.jsx)(n.code,{children:"RAGService"})," deployment. The ",(0,i.jsx)(n.code,{children:"RAGService"})," provides the following capabilities:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Orchestration"}),": use ",(0,i.jsx)(n.a,{href:"https://github.com/run-llama/llama_index",children:"LlamaIndex"})," orchestrator."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding"}),": support both local and remote embedding services, to embed queries and documents in the vector database."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vector database"}),": support a built-in ",(0,i.jsx)(n.a,{href:"https://github.com/facebookresearch/faiss",children:"faiss"})," in-memory vector database. Remote vector database support will be added soon."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Backend inference"}),": support any OAI compatible inference service."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The details of the service APIs can be found in this ",(0,i.jsx)(n.a,{href:"/kaito/docs/next/rag",children:"document"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,i.jsxs)(n.p,{children:["\ud83d\udc49 To get started, please see the ",(0,i.jsx)(n.a,{href:"installation",children:"Workspace Installation Guide"})," and the ",(0,i.jsx)(n.a,{href:"/kaito/docs/next/rag",children:"RAGEngine Installation Guide"}),"!"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83d\udc49 For a quick start tutorial, check out ",(0,i.jsx)(n.a,{href:"quick-start",children:"Quick Start"}),"!"]}),"\n",(0,i.jsx)(n.h2,{id:"community",children:"Community"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GitHub"}),": ",(0,i.jsx)(n.a,{href:"https://github.com/kaito-project/kaito",children:"kaito-project/kaito"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Slack"}),": ",(0,i.jsx)(n.a,{href:"https://cloud-native.slack.com/archives/C09B4EWCZ5M",children:"Join #kaito channel in CNCF Slack"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Email"}),": ",(0,i.jsx)(n.a,{href:"mailto:kaito-dev@microsoft.com",children:"kaito-dev@microsoft.com"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var r=t(96540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},30679:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/arch-aabbcb084e64f19f934f9d05c360b519.png"},52325:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/ragarch-659d20a2e1252e38417fc591d434e7d1.png"}}]);