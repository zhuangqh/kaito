"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[528],{8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},8537:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var t=s(4848),i=s(8453);const r={title:"Distributed Inference",authors:["chewong"],reviewers:["KAITO contributor"],"creation-date":new Date("2025-03-25T00:00:00.000Z"),"last-updated":new Date("2025-05-01T00:00:00.000Z"),status:"provisional"},a="Title",o={id:"proposals/distributed-inference",title:"Distributed Inference",description:"Distributed Inference",source:"@site/docs/proposals/20250325-distributed-inference.md",sourceDirName:"proposals",slug:"/proposals/distributed-inference",permalink:"/kaito/docs/next/proposals/distributed-inference",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250325-distributed-inference.md",tags:[],version:"current",sidebarPosition:20250325,frontMatter:{title:"Distributed Inference",authors:["chewong"],reviewers:["KAITO contributor"],"creation-date":"2025-03-25T00:00:00.000Z","last-updated":"2025-05-01T00:00:00.000Z",status:"provisional"}},l={},d=[{value:"Summary",id:"summary",level:2},{value:"What Is Distributed Inference Anyway?",id:"what-is-distributed-inference-anyway",level:2},{value:"Goals",id:"goals",level:3},{value:"Non-Goals",id:"non-goals",level:3},{value:"Requirements",id:"requirements",level:2},{value:"Workspace API Changes",id:"workspace-api-changes",level:3},{value:"vLLM Runtime Parameters",id:"vllm-runtime-parameters",level:3},{value:"Liveness and Readiness Probes",id:"liveness-and-readiness-probes",level:3},{value:"Leader Pod Crash",id:"leader-pod-crash",level:4},{value:"Worker Pod Crash",id:"worker-pod-crash",level:4},{value:"Why Restart the Leader When a Worker Crashes Regardless of the State?",id:"why-restart-the-leader-when-a-worker-crashes-regardless-of-the-state",level:4},{value:"Example Configuration",id:"example-configuration",level:4},{value:"Container Image Update",id:"container-image-update",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"title",children:"Title"}),"\n",(0,t.jsx)(n.p,{children:"Distributed Inference"}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"While KAITO excels with single-GPU models, the increasing size of state-of-the-art models necessitates multi-node distributed inference capabilities. Models with hundreds of billions of parameters often exceed the memory capacity of even the largest single nodes available today."}),"\n",(0,t.jsxs)(n.p,{children:["The default vLLM runtime lacks multi-node support within KAITO since its adoption in January 2025 (",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/pull/823",children:"#823"}),"). This proposal aims to bridge this gap by implementing multi-node distributed inference, primarily focusing on the vLLM runtime. The goal is to enable  deployment of very large preset models across multiple nodes, ensuring KAITO remains capable of serving cutting-edge models while maintaining a consistent user experience."]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-distributed-inference-anyway",children:"What Is Distributed Inference Anyway?"}),"\n",(0,t.jsx)(n.p,{children:"Distributed inference enables models too large for a single GPU to run across multiple GPUs or nodes. Key strategies relevant to KAITO include:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Single-Node Multi-GPU (Tensor Parallelism):"})," Splits a model across multiple GPUs ",(0,t.jsx)(n.em,{children:"within"})," a single node. Used when a model fits on one node but exceeds a single GPU's memory. Both HuggingFace Transformers and vLLM support this."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Node Multi-GPU (Pipeline and Tensor Parallelism):"})," Splits a model across multiple nodes, typically using pipeline parallelism between nodes and tensor parallelism within each node. Used for models exceeding a single node's capacity. HuggingFace Transformers supports this via Torch Elastic, but vLLM currently does not in KAITO."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"KAITO's current support for these strategies:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Strategy"}),(0,t.jsx)(n.th,{style:{textAlign:"center"},children:"vLLM"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Single GPU"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Single-Node Multi-GPU"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Multi-Node Multi-GPU"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"No (This proposal)"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"goals",children:"Goals"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement multi-node distributed inference for preset models using the vLLM inference runtime in KAITO."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"non-goals",children:"Non-Goals"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Applying distributed inference to all preset models:"})," This proposal specifically targets very large models (e.g., 400B+ parameters) that necessitate multi-node deployment. Applying it to smaller models is not intended, as it could introduce unnecessary overhead and potentially degrade performance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Supporting distributed inference for custom models:"})," Custom models currently use Kubernetes Deployments, which lack the stable pod identity required for the proposed multi-node coordination using StatefulSets. Extending support to custom models would require a separate effort."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implementing distributed model tuning:"})," The scope of this proposal is limited to inference. Distributed tuning is not addressed."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Introducing new runtimes or deployment mechanisms:"})," This proposal focuses on enhancing the existing vLLM runtime and StatefulSet deployment strategy, rather than introducing alternatives like LeaderWorkerSet."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"workspace-api-changes",children:"Workspace API Changes"}),"\n",(0,t.jsx)(n.p,{children:"No API changes are needed for the workspace. The existing workspace API implicitly supports multi-node distributed inference, so users can continue using the same Workspace spec without any changes due to the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The number of nodes is specified via ",(0,t.jsx)(n.code,{children:".resource.count"})," in the Workspace specification:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'...\nresource:\n  count: 2\n  instanceType: "Standard_ND96asr_v4"\n...\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For pre-provisioned nodes, users may define a list in the existing API ",(0,t.jsx)(n.code,{children:".resource.preferredNodes"})," to ensure inference deployment to specific nodes:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"...\nresource:\n  count: 2\n  preferredNodes:\n    - my-favorite-node-1\n    - my-favorite-node-2\n...\n"})}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","For models requiring more than one GPU, validate that ",(0,t.jsx)(n.code,{children:"# GPUs/instance \xd7 workspace.resource.count \u2265 Required # GPUs for a preset model"}),", and ",(0,t.jsx)(n.code,{children:"GPU memory * # GPUs \u2265 Required model memory"}),". This validation is performed in the API server when creating or updating a workspace. If the condition is not met, an error message will be returned to the user. This is already implemented in ",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/blob/1815428804593eaa94de0d6f78d82b53e85d0137/api/v1beta1/workspace_validation.go#L293-L380",children:(0,t.jsx)(n.code,{children:"api/v1beta1/workspace_validation.go"})})," and does not require any changes."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["KAITO respects the user-defined ",(0,t.jsx)(n.code,{children:"workspace.resource.count"})," and creates exactly that number of nodes in the Workspace through gpu-provisioner. However, based on the chosen instance type, KAITO may deploy the model on fewer nodes than ",(0,t.jsx)(n.code,{children:"workspace.resource.count"})," if the model can fit in fewer nodes. This behavior is intentional to optimize GPU utilization and reduce overall inter-node communication overhead. Users should be mindful of the cost incurred when specifying a high ",(0,t.jsx)(n.code,{children:"workspace.resource.count"})," if the model can fit in fewer nodes."]})}),"\n",(0,t.jsx)(n.h3,{id:"vllm-runtime-parameters",children:"vLLM Runtime Parameters"}),"\n",(0,t.jsxs)(n.p,{children:["Flag additions to the vLLM base command is needed to support multi-node distributed inference. Per vLLM's ",(0,t.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/serving/distributed_serving.html",children:"guidance"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,t.jsx)(n.code,{children:"--tensor-parallel-size"}),": Set to the number of GPUs per node to configure tensor parallelism. This parameter determines how the model\u2019s tensors are split across GPUs within a single node."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.code,{children:"--pipeline-parallel-size"}),": Set to the number of nodes to configure pipeline parallelism. This parameter determines how different model layers are sharded across nodes."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["vLLM uses ",(0,t.jsx)(n.a,{href:"https://www.ray.io/",children:"Ray"})," as the default framework to manage its distributed inference runtime. vLLM has provided a convenient script called ",(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm/blob/main/examples/online_serving/multi-node-serving.sh",children:(0,t.jsx)(n.code,{children:"multi-node-serving.sh"})})," to start the Ray server. The same script can be used by worker pods to join the Ray cluster."]}),"\n",(0,t.jsx)(n.p,{children:"Leader:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"...\ncommand:\n  - /bin/sh\n  - -c\n  - |\n    # --ray_cluster_size is the number of nodes in the cluster\n    /workspace/vllm/multi-node-serving.sh leader --ray_cluster_size=2 --ray_port=6379\n    # 8 GPUs per node, 2 nodes\n    python3 /workspace/vllm/inference_api.py --tensor-parallel-size=8 --pipeline-parallel-size=2 --served-model-name=super-huge-model --kaito-config-file=/mnt/config/inference_config.yaml\n...\n"})}),"\n",(0,t.jsx)(n.p,{children:"Workers:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"...\ncommand:\n  - /bin/sh\n  - -c\n  - |\n    # --ray_address points to the cluster IP of the headless service of the leader pod\n    /workspace/vllm/multi-node-serving.sh worker --ray_address=http://10.1.2.3:6379\n...\n"})}),"\n",(0,t.jsxs)(n.p,{children:["With a StatefulSet, the leader pod can be identified by index ",(0,t.jsx)(n.code,{children:"0"})," in the pod name (e.g., ",(0,t.jsx)(n.code,{children:"kaito-vllm-0"}),"), and the worker pod index can be identified by their ordinal indices (e.g., ",(0,t.jsx)(n.code,{children:"kaito-vllm-1"}),", ",(0,t.jsx)(n.code,{children:"kaito-vllm-2"}),", etc.). The Ray cluster address can be constructed using the headless service of the StatefulSet."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'...\ncommand:\n  - /bin/sh\n  - -c\n  - |\n    if [ "${POD_INDEX}" = "0" ]; then\n      <leader command>\n    else\n      <worker command>\n    fi\nenv:\n  - name: POD_INDEX\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.labels[\'apps.kubernetes.io/pod-index\']\n...\n'})}),"\n",(0,t.jsx)(n.h3,{id:"liveness-and-readiness-probes",children:"Liveness and Readiness Probes"}),"\n",(0,t.jsx)(n.p,{children:"Standard HTTP probes are insufficient for multi-node vLLM, as only the leader pod serves the /health endpoint at port 5000, and Ray cluster dynamics in Kubernetes introduce significant complexities. Key challenges include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Leader Dependency and Initialization"}),": The leader waits for all n-1 worker pods to join the Ray cluster (where n is the StatefulSet replica count) before starting tasks like model downloading and loading it to GPU memory. If a worker pod fails permanently during initialization or restarts later, the leader does not reinitialize the new worker, rendering the vLLM service unusable until the leader is restarted to recreate the Ray cluster from a clean state."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Worker Rejoining Delay:"})," In the case of a leader restart, existing worker pods may take 10-30 seconds to rejoin the new Ray cluster."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ray Actor Health & Count Validation"}),": The Ray cluster\u2019s health depends on having enough alive actors matching the world size. Dead or missing actors must be detected."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Failure Intolerance"}),": The current design does not tolerate worker or leader pod failures gracefully. Although leader restart does not require all workers to restart, each worker restart requires a leader restart for synchronization purposes, and sequential node upgrades may incur significant downtime."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"To address these issues, the following probing strategy is proposed:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{}),(0,t.jsx)(n.th,{children:"Liveness Probe (Triggers Container Restart on Failure)"}),(0,t.jsx)(n.th,{children:"Readiness Probe (Does Not Trigger Container Restart on Failure)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Leader"})}),(0,t.jsx)(n.td,{children:"Verifies Ray cluster health by querying the Ray dashboard API for the inference workload's job ID and confirming no dead actors associated with that job. If dead actors are found, the probe fails, triggering a container restart to reinitialize the Ray cluster."}),(0,t.jsxs)(n.td,{children:["Check ",(0,t.jsx)(n.code,{children:"$(LEADER_HEADLESS_SVC):$(VLLM_PORT)/health"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Worker"})}),(0,t.jsxs)(n.td,{children:["vLLM does not gracefully handle worker pod restarts (see ",(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm/issues/16259",children:"vLLM #16259"}),"), as a restarted worker joins the existing Ray cluster but remains idle, rendering the inference service unusable despite the Ray cluster appearing healthy. That said, implementing a liveness probe for worker pods is unnecessary, as worker health is indirectly validated through the leader\u2019s liveness probe. The leader\u2019s probe, by detecting dead actors, triggers a complete Ray cluster reinitialization, synchronizing both leader and worker pods to restore service functionality."]}),(0,t.jsxs)(n.td,{children:["Check ",(0,t.jsx)(n.code,{children:"$(LEADER_HEADLESS_SVC):$(VLLM_PORT)/health"}),"."]})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The following sequential diagram summarizes the expected behavior of the liveness and readiness probes during different failure scenarios:"}),"\n",(0,t.jsx)(n.h4,{id:"leader-pod-crash",children:"Leader Pod Crash"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant P as Readiness Probes\n    participant L as Leader Pod\n    participant W as Worker Pod\n    participant R as Ray Cluster\n    participant V as Inference Service\n\n    L--xL: Crashes\n    Note over V: Becomes unavailable (no leader)\n    L--xL: Restarts\n    P->>W: Checks /health endpoint from leader\n    activate W\n    W->>P: Fails, marks as not ready\n    deactivate W\n    L->>R: Recreates\n    Note over W: May heartbeat to old leader\n    W->>R: Joins new cluster (10-30 seconds delay)\n    L->>R: Waits for all workers to join\n    activate R\n    R->>L: All workers join\n    deactivate R\n    Note over L: vLLM Initialization Phase: Model downloading and loading to GPU memory\n    Note over V: Becomes available again\n"})}),"\n",(0,t.jsx)(n.h4,{id:"worker-pod-crash",children:"Worker Pod Crash"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n  participant K as Liveness Probes\n  participant P as Readiness Probes\n  participant L as Leader Pod\n  participant W as Worker Pod\n  participant R as Ray Cluster\n  participant V as Inference Service\n\n  W--xW: Crashes\n  Note over V: Hangs (insufficient workers)\n  P->>L: Checks /health endpoint\n  activate L\n  L->>P: Succeeds due to vLLM's assumption of all nodes being healthy\n  deactivate L\n  W--xW: Restarts\n  W->>R: Rejoins but remains idle\n  Note over V: Continues to hang due to uninitialized worker\n  K->>L: Checks for dead Ray actors\n  activate L\n  L->>K: Fails, triggers leader container restart\n  deactivate L\n  L--xL: Restarts\n  L->>R: Recreates\n  Note over W: May heartbeat to old leader\n  W->>R: Joins new cluster (10-30 seconds delay)\n  L->>R: Waits for all workers to join\n  activate R\n  R->>L: All workers join\n  deactivate R\n  Note over L,W: vLLM Initialization Phase: Model downloading and loading to GPU memory\n  Note over V: Becomes available again\n"})}),"\n",(0,t.jsx)(n.h4,{id:"why-restart-the-leader-when-a-worker-crashes-regardless-of-the-state",children:"Why Restart the Leader When a Worker Crashes Regardless of the State?"}),"\n",(0,t.jsx)(n.p,{children:"Restarting the leader is important for reinitializing the Ray cluster and synchronizing it with any restarted worker pods. This is due to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"vLLM's Fault Tolerance Assumption"}),": In a distributed setup, vLLM operates under the assumption that all nodes remain healthy and available (",(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm/blob/d43f914d42dc00a59ca8b6d26363cf02b3b898b2/vllm/executor/ray_distributed_executor.py#L697-L700",children:"source"}),"). If a worker pod fails, the leader might not correctly recognize this change in the cluster's state, potentially leading to operational inconsistencies."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State Synchronization"}),": While restarted worker pods might rejoin the existing Ray cluster, they may not share the same operational state as other pods (e.g., model weights loaded into GPU memory). A leader restart ensures that all pods are synchronized and operate with a consistent state."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"example-configuration",children:"Example Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Example configuration for the liveness and readiness probes:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"livenessProbe:\n  exec:\n    command:\n      - /bin/sh\n      - -c\n      - python3 /workspace/vllm/multi-node-health-check.py liveness --leader-address $(LEADER_HEADLESS_SVC) --ray-port 6379\n  initialDelaySeconds: 60\n  periodSeconds: 10\n  failThreshold: 1\nreadinessProbe:\n  exec:\n    command:\n      - /bin/sh\n      - -c\n      - python3 /workspace/vllm/multi-node-health-check.py readiness --leader-address $(LEADER_HEADLESS_SVC) --vllm-port 5000\n  periodSeconds: 10\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The health check script can use the environment variable ",(0,t.jsx)(n.code,{children:"POD_INDEX"})," to determine if the pod is a leader or worker. The probe should also include an initial delay to the liveness probe to allow time for the leader to initialize the Ray cluster and wait for all worker pods to join. This delay is necessary to avoid false positives during the initial startup phase. Failure thresholds should be set to 1 for liveness probes to ensure that any failure in the leader pod triggers an immediate restart instead of waiting for multiple failures."]}),"\n",(0,t.jsx)(n.h3,{id:"container-image-update",children:"Container Image Update"}),"\n",(0,t.jsx)(n.p,{children:"The preset model container images require the following updates:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Include Essential Scripts:"})," Add the ",(0,t.jsx)(n.code,{children:"multi-node-serving.sh"})," script (sourced from ",(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm/blob/main/examples/online_serving/multi-node-serving.sh",children:"vLLM examples"}),") and the custom ",(0,t.jsx)(n.code,{children:"multi-node-health-check.py"})," script to the Dockerfile. These scripts are necessary for initializing the Ray cluster and performing health checks in a multi-node environment."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Handle Large Model Weights:"})," For very large models (e.g., 400B+ parameters), embedding weights directly into the container image is impractical due to size constraints. Leverage existing mechanisms for runtime model weight downloading (",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/issues/982",children:"#982"}),") or external weight caching (",(0,t.jsx)(n.a,{href:"https://github.com/kaito-project/kaito/issues/1023",children:"#1023"}),") to manage these large artifacts effectively."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);