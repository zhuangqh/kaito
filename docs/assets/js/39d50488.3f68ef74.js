"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[2654],{4172:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":"unreleased","badge":true,"noIndex":false,"className":"docs-version-current","isLast":false,"docsSidebars":{"sidebar":[{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"link","label":"Introduction","href":"/kaito/docs/next/","docId":"intro","unlisted":false},{"type":"link","label":"Installation","href":"/kaito/docs/next/installation","docId":"installation","unlisted":false},{"type":"link","label":"Quick Start","href":"/kaito/docs/next/quick-start","docId":"quick-start","unlisted":false},{"type":"link","label":"Presets","href":"/kaito/docs/next/presets","docId":"presets","unlisted":false},{"type":"link","label":"Usage","href":"/kaito/docs/next/usage","docId":"usage","unlisted":false},{"type":"link","label":"FAQ","href":"/kaito/docs/next/faq","docId":"faq","unlisted":false}],"collapsible":true},{"type":"category","label":"Cloud Providers","collapsed":false,"items":[{"type":"link","label":"Azure Setup","href":"/kaito/docs/next/azure","docId":"azure","unlisted":false},{"type":"link","label":"AWS Setup","href":"/kaito/docs/next/aws","docId":"aws","unlisted":false}],"collapsible":true},{"type":"category","label":"Features","collapsed":false,"items":[{"type":"link","label":"Inference","href":"/kaito/docs/next/inference","docId":"inference","unlisted":false},{"type":"link","label":"Multi-Node Inference","href":"/kaito/docs/next/multi-node-inference","docId":"multi-node-inference","unlisted":false},{"type":"link","label":"Fine Tuning","href":"/kaito/docs/next/tuning","docId":"tuning","unlisted":false},{"type":"category","label":"Retrieval-Augmented Generation (RAG)","items":[{"type":"link","label":"API Definitions and Examples","href":"/kaito/docs/next/rag-api","docId":"rag-api","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/kaito/docs/next/rag"},{"type":"link","label":"Custom Model Integration","href":"/kaito/docs/next/custom-model","docId":"custom-model","unlisted":false},{"type":"link","label":"Tool Calling","href":"/kaito/docs/next/tool-calling","docId":"tool-calling","unlisted":false},{"type":"link","label":"Model As OCI Artifacts","href":"/kaito/docs/next/model-as-oci-artifacts","docId":"model-as-oci-artifacts","unlisted":false},{"type":"link","label":"Headlamp KAITO","href":"/kaito/docs/next/headlamp-kaito","docId":"headlamp-kaito","unlisted":false}],"collapsible":true},{"type":"category","label":"Integrations","collapsed":false,"items":[{"type":"link","label":"AIKit Integration with KAITO","href":"/kaito/docs/next/aikit","docId":"aikit","unlisted":false}],"collapsible":true},{"type":"category","label":"Operations","collapsed":false,"items":[{"type":"link","label":"Monitoring","href":"/kaito/docs/next/monitoring","docId":"monitoring","unlisted":false},{"type":"link","label":"OOM Prevention","href":"/kaito/docs/next/kaito-oom-prevention","docId":"kaito-oom-prevention","unlisted":false},{"type":"link","label":"Bring Your Own GPU Nodes","href":"/kaito/docs/next/kaito-on-byo-gpu-nodes","docId":"kaito-on-byo-gpu-nodes","unlisted":false},{"type":"link","label":"GPU Benchmarks","href":"/kaito/docs/next/gpu-benchmarks","docId":"gpu-benchmarks","unlisted":false}],"collapsible":true},{"type":"category","label":"Contributing","collapsed":false,"items":[{"type":"link","label":"Contributing","href":"/kaito/docs/next/contributing","docId":"contributing","unlisted":false},{"type":"link","label":"Preset onboarding","href":"/kaito/docs/next/preset-onboarding","docId":"preset-onboarding","unlisted":false},{"type":"link","label":"Proposals","href":"/kaito/docs/next/proposals","docId":"proposals","unlisted":false}],"collapsible":true}]},"docs":{"aikit":{"id":"aikit","title":"AIKit Integration with KAITO","description":"AIKit provides a streamlined way to package and deploy large language models (LLMs) as container images.","sidebar":"sidebar"},"aws":{"id":"aws","title":"AWS Setup","description":"This guide covers setting up auto-provisioning capabilities for KAITO on Amazon Elastic Kubernetes Service (EKS). Auto-provisioning allows KAITO to automatically create GPU nodes when needed for your AI workloads.","sidebar":"sidebar"},"azure":{"id":"azure","title":"Azure Setup","description":"This guide covers setting up auto-provisioning capabilities for KAITO on Azure Kubernetes Service (AKS). Auto-provisioning allows KAITO to automatically create GPU nodes when needed for your AI workloads.","sidebar":"sidebar"},"contributing":{"id":"contributing","title":"Contributing","description":"This project welcomes contributions and suggestions.","sidebar":"sidebar"},"custom-model":{"id":"custom-model","title":"Custom Model Integration","description":"Option 1: Use Pre-Built Docker Image Without Model Weights","sidebar":"sidebar"},"faq":{"id":"faq","title":"FAQ","description":"How do I ensure preferred nodes are correctly labeled for use in my workspace?","sidebar":"sidebar"},"gpu-benchmarks":{"id":"gpu-benchmarks","title":"GPU Benchmarks","description":"These benchmarks help users choose the optimal GPU SKU for running their AI models with Kaito. We compare performance characteristics across different GPU types to guide cost-effective hardware selection.","sidebar":"sidebar"},"headlamp-kaito":{"id":"headlamp-kaito","title":"Headlamp KAITO","description":"Headlamp","sidebar":"sidebar"},"inference":{"id":"inference","title":"Inference","description":"This document presents how to use the KAITO workspace Custom Resource Definition (CRD) for model serving and serving with LoRA adapters.","sidebar":"sidebar"},"installation":{"id":"installation","title":"Installation","description":"KAITO (Kubernetes AI Toolchain Operator) can be installed on any Kubernetes cluster using Helm. This guide covers the basic installation of the KAITO workspace controller.","sidebar":"sidebar"},"intro":{"id":"intro","title":"Introduction","description":"Retrieval Augmented Generation (RAG) support is live! - KAITO RagEngine uses LlamaIndex and FAISS, learn about it here!","sidebar":"sidebar"},"kaito-on-byo-gpu-nodes":{"id":"kaito-on-byo-gpu-nodes","title":"Bring Your Own GPU Nodes","description":"This guide walks you through deploying KAITO on a Kubernetes cluster with self-provisioned GPU nodes.","sidebar":"sidebar"},"kaito-oom-prevention":{"id":"kaito-oom-prevention","title":"OOM Prevention","description":"Large Language Model (LLM) inference typically requires significant GPU memory. Unlike traditional virtual memory management in operating systems, CUDA does not support GPU memory swapping. This limitation often leads to CUDA Out-Of-Memory (OOM) errors when GPU memory is insufficient for inference requests. Addressing GPU OOM issues is one of the most challenging aspects of LLM deployment and tuning.","sidebar":"sidebar"},"model-as-oci-artifacts":{"id":"model-as-oci-artifacts","title":"Model As OCI Artifacts","description":"Efficiently distribute Large Language Models using Open Container Initiative (OCI) Artifacts","sidebar":"sidebar"},"monitoring":{"id":"monitoring","title":"Monitoring","description":"Inference service exposes Prometheus metrics for monitoring system stats.","sidebar":"sidebar"},"multi-node-inference":{"id":"multi-node-inference","title":"Multi-Node Inference","description":"This document explains how to configure and use multi-node distributed inference in KAITO for large models that require more GPU resources than a single node can provide.","sidebar":"sidebar"},"preset-onboarding":{"id":"preset-onboarding","title":"Preset onboarding","description":"This document describes how to add a new supported OSS model in KAITO. The process is designed to allow community users to initiate the request. KAITO maintainers will follow up and deal with managing the model images and guiding the code changes to set up the model preset configurations.","sidebar":"sidebar"},"presets":{"id":"presets","title":"Presets","description":"The current supported model families with preset configurations are listed below.","sidebar":"sidebar"},"proposals":{"id":"proposals","title":"Proposals","description":"This section contains proposals for adding new models to KAITO. Each proposal describes the process of evaluating and integrating new OSS models into the KAITO ecosystem.","sidebar":"sidebar"},"quick-start":{"id":"quick-start","title":"Quick Start","description":"After installing KAITO, you can quickly deploy a phi-4-mini-instruct inference service to get started.","sidebar":"sidebar"},"rag":{"id":"rag","title":"Retrieval-Augmented Generation (RAG)","description":"This document presents how to use the KAITO ragengine Custom Resource Definition (CRD) for retrieval-augumented generatoin workflow. By creating a RAGEngine resource, you can quickly stand up a service that indexes documents and queries them in conjunction with an existing LLM inference endpoint\u2014no need to custom-build pipelines. This enables your large language model to answer questions based on your own private content.","sidebar":"sidebar"},"rag-api":{"id":"rag-api","title":"API Definitions and Examples","description":"A RAGEngine index is a logical collection that organizes and stores your documents for retrieval-augmented generation workflows. The relationship between indexes, documents, and document nodes is as follows:","sidebar":"sidebar"},"tool-calling":{"id":"tool-calling","title":"Tool Calling","description":"KAITO supports tool calling, allowing you to integrate external tools into your inference service. This feature enables the model to call APIs or execute functions based on the input it receives, enhancing its capabilities beyond text generation.","sidebar":"sidebar"},"tuning":{"id":"tuning","title":"Fine Tuning","description":"This document presents how to use the KAITO workspace Custom Resource Definition (CRD) for parameter-efficient fine-tuning (PEFT) of models, how a Kubernetes job is designed to automate the tuning workflow, and several best practices for troubleshooting.","sidebar":"sidebar"},"usage":{"id":"usage","title":"Usage","description":"The detailed usage for Kaito supported models can be found in HERE. In case users want to deploy their own containerized models, they can provide the pod template in the inference field of the workspace custom resource (please see API definitions for details). The controller will create a deployment workload using all provisioned GPU nodes. Note that currently the controller does NOT handle automatic model upgrade. It only creates inference workloads based on the preset configurations if the workloads do not exist.","sidebar":"sidebar"}}}}')}}]);