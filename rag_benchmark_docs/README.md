# RAG Benchmark for Documents

This folder contains tools to benchmark RAG performance on **document-based Q&A** tasks.

> **Note**: This is specifically for testing RAG on text documents (PDFs, reports, manuals). Code-related RAG benchmarking uses a different tool.

## ğŸ“ Files

- **`rag_benchmark_docs.py`** - Main benchmark script
- **`RAG_BENCHMARK_DOCS_README.md`** - Quick start guide
- **`RAG_BENCHMARK_DOCS_GUIDE.md`** - Complete documentation

## ğŸš€ Quick Start

Read `RAG_BENCHMARK_DOCS_README.md` to get started in 2 minutes.

## ğŸ“Š What This Tests

- **Document retrieval accuracy**: How well RAG finds relevant context
- **Answer quality**: RAG vs pure LLM on factual questions
- **Comprehension**: RAG vs pure LLM on analytical questions
- **Token efficiency**: Cost comparison



Results are saved to `../benchmark_results/` directory.
