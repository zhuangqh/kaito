deployment:
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: MODEL_NAME
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: MODEL_NAME
    template:
      metadata:
        labels:
          app: MODEL_NAME
      spec:
        containers:
        - name: MODEL_NAME-container
          image: REPO.azurecr.io/MODEL_NAME:TAG
          command:
            - /bin/sh
            - -c
            - RUNTIME_COMMAND
          resources:
            requests:
              nvidia.com/gpu: GPU_COUNT
            limits:
              nvidia.com/gpu: GPU_COUNT
          livenessProbe:
            httpGet:
              path: /health
              port: 5000
            initialDelaySeconds: 600 # 10 Min
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 5000
            initialDelaySeconds: 30
            periodSeconds: 10
          volumeMounts:
          - name: dshm
            mountPath: /dev/shm
          - name: config-volume
            mountPath: /mnt/config
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: config-volume
          configMap:
            defaultMode: 420
            name: testing-inference-params
        tolerations:
        - effect: NoSchedule
          key: sku
          operator: Equal
          value: gpu
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
        nodeSelector:
          pool: NODE_POOL

statefulset:
  apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    name: MODEL_NAME
  spec:
    replicas: NODE_COUNT
    selector:
      matchLabels:
        app: MODEL_NAME
    podManagementPolicy: Parallel
    template:
      metadata:
        labels:
          app: MODEL_NAME
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - MODEL_NAME
              topologyKey: "kubernetes.io/hostname"
        containers:
          - name: MODEL_NAME-container
            image: REPO.azurecr.io/MODEL_NAME:TAG
            command:
              - /bin/sh
              - -c
              - RUNTIME_COMMAND
            resources:
              limits:
                nvidia.com/gpu: "GPU_COUNT"
              requests:
                nvidia.com/gpu: "GPU_COUNT"
            livenessProbe:
              httpGet:
                path: /health
                port: 5000
              initialDelaySeconds: 600 # 10 Min
              periodSeconds: 10
            readinessProbe:
              httpGet:
                path: /health
                port: 5000
              initialDelaySeconds: 30
              periodSeconds: 10
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
        tolerations:
          - effect: NoSchedule
            key: sku
            operator: Equal
            value: gpu
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
        nodeSelector:
          pool: NODE_POOL

config:
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: testing-inference-params
  data:
    inference_config.yaml: |
      # Maximum number of steps to find the max available seq len fitting in the GPU memory.
      max_probe_steps: 6

      vllm:
        cpu-offload-gb: 0
        gpu-memory-utilization: 0.95
        swap-space: 4
        served-model-name: test
        dtype: float16

        # max-seq-len-to-capture: 8192
        # num-scheduler-steps: 1
        # enable-chunked-prefill: false
        # see https://docs.vllm.ai/en/stable/models/engine_args.html for more options.

service:
  apiVersion: v1
  kind: Service
  metadata:
    name: MODEL_NAME
  spec:
    ports:
    - port: 5000
      targetPort: 5000
      protocol: TCP
      name: http
    selector:
      app: MODEL_NAME
    type: ClusterIP